{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d92ac758-107f-468a-b59f-8ab5a0fa4d71",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Ethics for NLP: Spring 2022\n",
    "## Homework 3: Low Ressource Languages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fad41e5",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Imports and configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8531a96e",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# All import statements defined here\n",
    "# ----------------\n",
    "# !pip install torch\n",
    "# !pip install torchvision\n",
    "# !pip install torchtext==0.9.0\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.legacy import data\n",
    "from torchtext.legacy import datasets\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "\n",
    "# set a fixed seed for reproducibility\n",
    "SEED = 1234\n",
    "\n",
    "random.seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "# hyperparameters\n",
    "params = {   \n",
    "    \"embedding_dim\": 100,\n",
    "    \"hidden_dim\":128,\n",
    "    \"n_layers\":2,\n",
    "    \"bidirectional\": True,\n",
    "    \"dropout\":0.25,\n",
    "    \"batch_size\": 128\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5863c71d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b9489a0b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Note: do not change anything in this code,\n",
    "# it can lead to incorrect results in the final accuracy calculation\n",
    "# ----------------\n",
    "\n",
    "class BiLSTMPOSTagger(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        input_dim,\n",
    "        embedding_dim,\n",
    "        hidden_dim,\n",
    "        output_dim,\n",
    "        n_layers,\n",
    "        bidirectional,\n",
    "        dropout,\n",
    "        pad_idx,\n",
    "    ):\n",
    "\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim, padding_idx=pad_idx)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            embedding_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=n_layers,\n",
    "            bidirectional=bidirectional,\n",
    "            dropout=dropout if n_layers > 1 else 0,\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(hidden_dim * 2 if bidirectional else hidden_dim, output_dim)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, text):\n",
    "\n",
    "        # pass text through embedding layer\n",
    "        embedded = self.dropout(self.embedding(text))\n",
    "\n",
    "        # pass embeddings into LSTM\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "\n",
    "        # outputs holds the backward and forward hidden states in the final layer\n",
    "        # hidden and cell are the backward and forward hidden and cell states at the final time-step\n",
    "\n",
    "        # we use our outputs to make a prediction of what the tag should be\n",
    "        predictions = self.fc(self.dropout(outputs))\n",
    "\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac97373",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Training and evaluation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ee0d370",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# function used to train or evaluate model\n",
    "# depending on the input parameters\n",
    "\n",
    "def run(mode, lang, model_name):\n",
    "    print(\"Running model in {} mode with lang: {}\".format(mode, lang))\n",
    "    TEXT = data.Field(lower=True)\n",
    "    UD_TAGS = data.Field()\n",
    "\n",
    "    fields = ((\"text\", TEXT), (\"udtags\", UD_TAGS))\n",
    "\n",
    "    train_data, valid_data, test_data = datasets.UDPOS.splits(\n",
    "        fields=fields,\n",
    "        path=os.path.join(\"data\", lang),\n",
    "        train=\"{}-ud-train.conll\".format(lang),\n",
    "        validation=\"{}-ud-dev.conll\".format(lang),\n",
    "        test=\"{}-ud-test.conll\".format(lang),\n",
    "    )\n",
    "    MIN_FREQ = 2\n",
    "    print(os.path.join(\"data\", lang))\n",
    "\n",
    "    TEXT.build_vocab(train_data, min_freq=MIN_FREQ)\n",
    "    UD_TAGS.build_vocab(train_data)\n",
    "\n",
    "    if mode == \"train\":\n",
    "        print(f\"Unique tokens in TEXT vocabulary: {len(TEXT.vocab)}\")\n",
    "        print(f\"Unique tokens in UD_TAG vocabulary: {len(UD_TAGS.vocab)}\")\n",
    "        print()\n",
    "        print(f\"Number of training examples: {len(train_data)}\")\n",
    "        print(f\"Number of validation examples: {len(valid_data)}\")\n",
    "\n",
    "        print(f\"Number of tokens in the training set: {sum(TEXT.vocab.freqs.values())}\")\n",
    "\n",
    "    print(f\"Number of testing examples: {len(test_data)}\")\n",
    "\n",
    "    if mode == \"train\":\n",
    "        print(\"Tag\\t\\tCount\\t\\tPercentage\\n\")\n",
    "        for tag, count, percent in tag_percentage(UD_TAGS.vocab.freqs.most_common()):\n",
    "            print(f\"{tag}\\t\\t{count}\\t\\t{percent*100:4.1f}%\")\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(\n",
    "        (train_data, valid_data, test_data),\n",
    "        batch_size=params[\"batch_size\"],\n",
    "        device=device,\n",
    "    )\n",
    "\n",
    "    PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "    model = BiLSTMPOSTagger(\n",
    "        input_dim=len(TEXT.vocab),\n",
    "        embedding_dim=params[\"embedding_dim\"],\n",
    "        hidden_dim=params[\"hidden_dim\"],\n",
    "        output_dim=len(UD_TAGS.vocab),\n",
    "        n_layers=params[\"n_layers\"],\n",
    "        bidirectional=params[\"bidirectional\"],\n",
    "        dropout=params[\"dropout\"],\n",
    "        pad_idx=PAD_IDX,\n",
    "    )\n",
    "\n",
    "    if mode == \"train\":\n",
    "\n",
    "        def init_weights(m):\n",
    "            for name, param in m.named_parameters():\n",
    "                nn.init.normal_(param.data, mean=0, std=0.1)\n",
    "\n",
    "        def count_parameters(model):\n",
    "            return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "        model.apply(init_weights)\n",
    "        print(f\"The model has {count_parameters(model):,} trainable parameters\")\n",
    "        model.embedding.weight.data[PAD_IDX] = torch.zeros(params[\"embedding_dim\"])\n",
    "        optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    TAG_PAD_IDX = UD_TAGS.vocab.stoi[UD_TAGS.pad_token]\n",
    "    TAG_UNK_IDX = UD_TAGS.vocab.unk_index\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=TAG_PAD_IDX)\n",
    "\n",
    "    model = model.to(device)\n",
    "    criterion = criterion.to(device)\n",
    "\n",
    "    if mode == \"train\":\n",
    "        N_EPOCHS = 10\n",
    "        best_valid_loss = float(\"inf\")\n",
    "        for epoch in range(N_EPOCHS):\n",
    "            start_time = time.time()\n",
    "            train_loss, train_acc = train(\n",
    "                model,\n",
    "                train_iterator,\n",
    "                optimizer,\n",
    "                criterion,\n",
    "                TAG_PAD_IDX,\n",
    "                TAG_UNK_IDX,\n",
    "            )\n",
    "            valid_loss, valid_acc = evaluate(\n",
    "                model, valid_iterator, criterion, TAG_PAD_IDX, TAG_UNK_IDX\n",
    "            )\n",
    "            end_time = time.time()\n",
    "\n",
    "            epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "            if valid_loss < best_valid_loss:\n",
    "                best_valid_loss = valid_loss\n",
    "                torch.save(\n",
    "                    model.state_dict(), \"saved_models/{}.pt\".format(model_name)\n",
    "                )\n",
    "\n",
    "            print(f\"Epoch: {epoch+1:02} | Epoch Time: {epoch_mins}m {epoch_secs}s\")\n",
    "            print(f\"\\tTrain Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%\")\n",
    "            print(f\"\\t Val. Loss: {valid_loss:.3f} |  Val. Acc: {valid_acc*100:.2f}%\")\n",
    "\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(\"saved_models/{}.pt\".format(model_name)))\n",
    "    except Exception as e:\n",
    "        print(\n",
    "            \"Model file `{}` doesn't exist. You need to train the model by running this code in train mode.\".format(\n",
    "                \"saved_models/{}.pt\".format(model_name)\n",
    "            )\n",
    "        )\n",
    "        return\n",
    "\n",
    "    test_loss, test_acc = evaluate(\n",
    "        model, test_iterator, criterion, TAG_PAD_IDX, TAG_UNK_IDX\n",
    "    )\n",
    "    print(f\"Test Loss: {test_loss:.3f} |  Test Acc: {test_acc*100:.2f}%\")\n",
    "\n",
    "\n",
    "def tag_percentage(tag_counts):\n",
    "    total_count = sum([count for tag, count in tag_counts])\n",
    "    tag_counts_percentages = [\n",
    "        (tag, count, count / total_count) for tag, count in tag_counts\n",
    "    ]\n",
    "    return tag_counts_percentages\n",
    "\n",
    "\n",
    "def categorical_accuracy(preds, y, tag_pad_idx, tag_unk_idx):\n",
    "    max_preds = preds.argmax(\n",
    "        dim=1, keepdim=True\n",
    "    )\n",
    "    non_pad_elements = torch.nonzero((y != tag_pad_idx) & (y != tag_unk_idx))\n",
    "    correct = max_preds[non_pad_elements].squeeze(1).eq(y[non_pad_elements])\n",
    "    return correct.float().sum(), y[non_pad_elements].shape[0]\n",
    "\n",
    "\n",
    "def train(model, iterator, optimizer, criterion, tag_pad_idx, tag_unk_idx):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    epoch_n_label = 0\n",
    "\n",
    "    model.train()\n",
    "\n",
    "    for batch in iterator:\n",
    "\n",
    "        text = batch.text\n",
    "        tags = batch.udtags\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        predictions = model(text)\n",
    "\n",
    "        predictions = predictions.view(-1, predictions.shape[-1])\n",
    "        tags = tags.view(-1)\n",
    "\n",
    "        loss = criterion(predictions, tags)\n",
    "\n",
    "        correct, n_labels = categorical_accuracy(\n",
    "            predictions, tags, tag_pad_idx, tag_unk_idx\n",
    "        )\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "        epoch_correct += correct.item()\n",
    "        epoch_n_label += n_labels\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_correct / epoch_n_label\n",
    "\n",
    "\n",
    "def evaluate(model, iterator, criterion, tag_pad_idx, tag_unk_idx):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_correct = 0\n",
    "    epoch_n_label = 0\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "\n",
    "        for batch in iterator:\n",
    "\n",
    "            text = batch.text\n",
    "            tags = batch.udtags\n",
    "\n",
    "            predictions = model(text)\n",
    "\n",
    "            predictions = predictions.view(-1, predictions.shape[-1])\n",
    "            tags = tags.view(-1)\n",
    "\n",
    "            loss = criterion(predictions, tags)\n",
    "\n",
    "            correct, n_labels = categorical_accuracy(\n",
    "                predictions, tags, tag_pad_idx, tag_unk_idx\n",
    "            )\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "            epoch_correct += correct.item()\n",
    "            epoch_n_label += n_labels\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_correct / epoch_n_label\n",
    "\n",
    "\n",
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2282e8c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 1 (10 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc7b3c92",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Evaluate the model on the english data (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "50d7a911",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model in train mode with lang: en\n",
      "data\\en\n",
      "Unique tokens in TEXT vocabulary: 8854\n",
      "Unique tokens in UD_TAG vocabulary: 19\n",
      "\n",
      "Number of training examples: 12543\n",
      "Number of validation examples: 2002\n",
      "Number of tokens in the training set: 204586\n",
      "Number of testing examples: 2077\n",
      "Tag\t\tCount\t\tPercentage\n",
      "\n",
      "NOUN\t\t35315\t\t17.3%\n",
      "VERB\t\t27508\t\t13.4%\n",
      "PUNCT\t\t23680\t\t11.6%\n",
      "ADP\t\t17640\t\t 8.6%\n",
      "PRON\t\t17183\t\t 8.4%\n",
      "DET\t\t17148\t\t 8.4%\n",
      "PROPN\t\t12945\t\t 6.3%\n",
      "ADJ\t\t12475\t\t 6.1%\n",
      "ADV\t\t10551\t\t 5.2%\n",
      "AUX\t\t7895\t\t 3.9%\n",
      "CONJ\t\t6707\t\t 3.3%\n",
      "PART\t\t5564\t\t 2.7%\n",
      "NUM\t\t3999\t\t 2.0%\n",
      "SCONJ\t\t3842\t\t 1.9%\n",
      "X\t\t848\t\t 0.4%\n",
      "INTJ\t\t688\t\t 0.3%\n",
      "SYM\t\t598\t\t 0.3%\n",
      "The model has 1,521,067 trainable parameters\n",
      "Epoch: 01 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 1.728 | Train Acc: 45.41%\n",
      "\t Val. Loss: 0.902 |  Val. Acc: 76.52%\n",
      "Epoch: 02 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.506 | Train Acc: 84.06%\n",
      "\t Val. Loss: 0.546 |  Val. Acc: 87.76%\n",
      "Epoch: 03 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.305 | Train Acc: 90.56%\n",
      "\t Val. Loss: 0.466 |  Val. Acc: 89.70%\n",
      "Epoch: 04 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.237 | Train Acc: 92.65%\n",
      "\t Val. Loss: 0.431 |  Val. Acc: 90.40%\n",
      "Epoch: 05 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.202 | Train Acc: 93.71%\n",
      "\t Val. Loss: 0.417 |  Val. Acc: 90.66%\n",
      "Epoch: 06 | Epoch Time: 1m 2s\n",
      "\tTrain Loss: 0.179 | Train Acc: 94.36%\n",
      "\t Val. Loss: 0.402 |  Val. Acc: 91.06%\n",
      "Epoch: 07 | Epoch Time: 1m 3s\n",
      "\tTrain Loss: 0.162 | Train Acc: 94.88%\n",
      "\t Val. Loss: 0.402 |  Val. Acc: 90.96%\n",
      "Epoch: 08 | Epoch Time: 1m 3s\n",
      "\tTrain Loss: 0.148 | Train Acc: 95.31%\n",
      "\t Val. Loss: 0.393 |  Val. Acc: 91.08%\n",
      "Epoch: 09 | Epoch Time: 1m 3s\n",
      "\tTrain Loss: 0.135 | Train Acc: 95.70%\n",
      "\t Val. Loss: 0.394 |  Val. Acc: 91.17%\n",
      "Epoch: 10 | Epoch Time: 1m 1s\n",
      "\tTrain Loss: 0.125 | Train Acc: 95.97%\n",
      "\t Val. Loss: 0.396 |  Val. Acc: 91.05%\n",
      "Test Loss: 0.397 |  Test Acc: 91.80%\n"
     ]
    }
   ],
   "source": [
    "# english\n",
    "run('train','en','POSTagger_en')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "301e9b4d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### For the rest of this task, train and then evaluate the model on the trained data for the languages in each code cell (8 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7d126933",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model in train mode with lang: cs\n",
      "data\\cs\n",
      "Unique tokens in TEXT vocabulary: 38789\n",
      "Unique tokens in UD_TAG vocabulary: 19\n",
      "\n",
      "Number of training examples: 41559\n",
      "Number of validation examples: 9270\n",
      "Number of tokens in the training set: 719317\n",
      "Number of testing examples: 10148\n",
      "Tag\t\tCount\t\tPercentage\n",
      "\n",
      "NOUN\t\t175971\t\t24.5%\n",
      "PUNCT\t\t101318\t\t14.1%\n",
      "ADJ\t\t86855\t\t12.1%\n",
      "VERB\t\t79779\t\t11.1%\n",
      "ADP\t\t71491\t\t 9.9%\n",
      "PROPN\t\t46100\t\t 6.4%\n",
      "ADV\t\t37704\t\t 5.2%\n",
      "PRON\t\t34941\t\t 4.9%\n",
      "CONJ\t\t26714\t\t 3.7%\n",
      "NUM\t\t18004\t\t 2.5%\n",
      "SCONJ\t\t13186\t\t 1.8%\n",
      "DET\t\t12997\t\t 1.8%\n",
      "AUX\t\t9953\t\t 1.4%\n",
      "PART\t\t3825\t\t 0.5%\n",
      "SYM\t\t415\t\t 0.1%\n",
      "INTJ\t\t63\t\t 0.0%\n",
      "X\t\t1\t\t 0.0%\n",
      "The model has 4,514,567 trainable parameters\n",
      "Epoch: 01 | Epoch Time: 3m 2s\n",
      "\tTrain Loss: 0.848 | Train Acc: 72.14%\n",
      "\t Val. Loss: 0.270 |  Val. Acc: 91.87%\n",
      "Epoch: 02 | Epoch Time: 2m 59s\n",
      "\tTrain Loss: 0.179 | Train Acc: 94.18%\n",
      "\t Val. Loss: 0.182 |  Val. Acc: 94.22%\n",
      "Epoch: 03 | Epoch Time: 3m 3s\n",
      "\tTrain Loss: 0.113 | Train Acc: 96.30%\n",
      "\t Val. Loss: 0.163 |  Val. Acc: 94.77%\n",
      "Epoch: 04 | Epoch Time: 3m 2s\n",
      "\tTrain Loss: 0.091 | Train Acc: 96.96%\n",
      "\t Val. Loss: 0.153 |  Val. Acc: 94.92%\n",
      "Epoch: 05 | Epoch Time: 3m 5s\n",
      "\tTrain Loss: 0.078 | Train Acc: 97.41%\n",
      "\t Val. Loss: 0.155 |  Val. Acc: 95.05%\n",
      "Epoch: 06 | Epoch Time: 3m 2s\n",
      "\tTrain Loss: 0.067 | Train Acc: 97.76%\n",
      "\t Val. Loss: 0.157 |  Val. Acc: 95.05%\n",
      "Epoch: 07 | Epoch Time: 3m 14s\n",
      "\tTrain Loss: 0.058 | Train Acc: 98.08%\n",
      "\t Val. Loss: 0.160 |  Val. Acc: 95.03%\n",
      "Epoch: 08 | Epoch Time: 3m 5s\n",
      "\tTrain Loss: 0.050 | Train Acc: 98.34%\n",
      "\t Val. Loss: 0.168 |  Val. Acc: 95.04%\n",
      "Epoch: 09 | Epoch Time: 2m 59s\n",
      "\tTrain Loss: 0.043 | Train Acc: 98.58%\n",
      "\t Val. Loss: 0.175 |  Val. Acc: 94.98%\n",
      "Epoch: 10 | Epoch Time: 3m 3s\n",
      "\tTrain Loss: 0.037 | Train Acc: 98.78%\n",
      "\t Val. Loss: 0.187 |  Val. Acc: 94.97%\n",
      "Test Loss: 0.185 |  Test Acc: 94.41%\n"
     ]
    }
   ],
   "source": [
    "# czech\n",
    "run('train','cs','POSTagger_cs')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b9f61c83",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model in train mode with lang: es\n",
      "data\\es\n",
      "Unique tokens in TEXT vocabulary: 17588\n",
      "Unique tokens in UD_TAG vocabulary: 18\n",
      "\n",
      "Number of training examples: 14187\n",
      "Number of validation examples: 1552\n",
      "Number of tokens in the training set: 382436\n",
      "Number of testing examples: 274\n",
      "Tag\t\tCount\t\tPercentage\n",
      "\n",
      "NOUN\t\t68694\t\t18.0%\n",
      "ADP\t\t62995\t\t16.5%\n",
      "DET\t\t53937\t\t14.1%\n",
      "PUNCT\t\t42218\t\t11.0%\n",
      "VERB\t\t36185\t\t 9.5%\n",
      "PROPN\t\t35112\t\t 9.2%\n",
      "ADJ\t\t22096\t\t 5.8%\n",
      "PRON\t\t12402\t\t 3.2%\n",
      "CONJ\t\t12262\t\t 3.2%\n",
      "ADV\t\t11031\t\t 2.9%\n",
      "NUM\t\t9812\t\t 2.6%\n",
      "SCONJ\t\t7095\t\t 1.9%\n",
      "AUX\t\t5335\t\t 1.4%\n",
      "X\t\t1778\t\t 0.5%\n",
      "SYM\t\t1452\t\t 0.4%\n",
      "PART\t\t32\t\t 0.0%\n",
      "The model has 2,394,210 trainable parameters\n",
      "Epoch: 01 | Epoch Time: 1m 35s\n",
      "\tTrain Loss: 1.375 | Train Acc: 56.23%\n",
      "\t Val. Loss: 0.528 |  Val. Acc: 83.40%\n",
      "Epoch: 02 | Epoch Time: 1m 35s\n",
      "\tTrain Loss: 0.403 | Train Acc: 87.47%\n",
      "\t Val. Loss: 0.264 |  Val. Acc: 91.74%\n",
      "Epoch: 03 | Epoch Time: 1m 34s\n",
      "\tTrain Loss: 0.252 | Train Acc: 92.17%\n",
      "\t Val. Loss: 0.216 |  Val. Acc: 93.09%\n",
      "Epoch: 04 | Epoch Time: 1m 37s\n",
      "\tTrain Loss: 0.204 | Train Acc: 93.61%\n",
      "\t Val. Loss: 0.201 |  Val. Acc: 93.48%\n",
      "Epoch: 05 | Epoch Time: 1m 39s\n",
      "\tTrain Loss: 0.177 | Train Acc: 94.38%\n",
      "\t Val. Loss: 0.194 |  Val. Acc: 93.79%\n",
      "Epoch: 06 | Epoch Time: 1m 39s\n",
      "\tTrain Loss: 0.159 | Train Acc: 94.86%\n",
      "\t Val. Loss: 0.190 |  Val. Acc: 93.76%\n",
      "Epoch: 07 | Epoch Time: 1m 37s\n",
      "\tTrain Loss: 0.147 | Train Acc: 95.28%\n",
      "\t Val. Loss: 0.189 |  Val. Acc: 93.90%\n",
      "Epoch: 08 | Epoch Time: 1m 39s\n",
      "\tTrain Loss: 0.136 | Train Acc: 95.58%\n",
      "\t Val. Loss: 0.188 |  Val. Acc: 93.99%\n",
      "Epoch: 09 | Epoch Time: 1m 40s\n",
      "\tTrain Loss: 0.127 | Train Acc: 95.86%\n",
      "\t Val. Loss: 0.190 |  Val. Acc: 93.99%\n",
      "Epoch: 10 | Epoch Time: 1m 39s\n",
      "\tTrain Loss: 0.118 | Train Acc: 96.16%\n",
      "\t Val. Loss: 0.192 |  Val. Acc: 94.00%\n",
      "Test Loss: 0.213 |  Test Acc: 94.00%\n"
     ]
    }
   ],
   "source": [
    "# spanish\n",
    "run('train','es','POSTagger_spanish')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "28ce802b",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model in train mode with lang: ar\n",
      "data\\ar\n",
      "Unique tokens in TEXT vocabulary: 15889\n",
      "Unique tokens in UD_TAG vocabulary: 18\n",
      "\n",
      "Number of training examples: 6174\n",
      "Number of validation examples: 786\n",
      "Number of tokens in the training set: 225853\n",
      "Number of testing examples: 704\n",
      "Tag\t\tCount\t\tPercentage\n",
      "\n",
      "NOUN\t\t74156\t\t32.8%\n",
      "ADP\t\t33548\t\t14.9%\n",
      "ADJ\t\t23424\t\t10.4%\n",
      "CONJ\t\t19182\t\t 8.5%\n",
      "PUNCT\t\t17777\t\t 7.9%\n",
      "X\t\t17626\t\t 7.8%\n",
      "VERB\t\t17175\t\t 7.6%\n",
      "PRON\t\t10904\t\t 4.8%\n",
      "NUM\t\t6191\t\t 2.7%\n",
      "PART\t\t2996\t\t 1.3%\n",
      "DET\t\t1537\t\t 0.7%\n",
      "ADV\t\t827\t\t 0.4%\n",
      "SYM\t\t316\t\t 0.1%\n",
      "PROPN\t\t156\t\t 0.1%\n",
      "AUX\t\t31\t\t 0.0%\n",
      "INTJ\t\t7\t\t 0.0%\n",
      "The model has 2,224,310 trainable parameters\n",
      "Epoch: 01 | Epoch Time: 1m 23s\n",
      "\tTrain Loss: 2.016 | Train Acc: 35.50%\n",
      "\t Val. Loss: 1.463 |  Val. Acc: 55.48%\n",
      "Epoch: 02 | Epoch Time: 1m 21s\n",
      "\tTrain Loss: 0.868 | Train Acc: 72.48%\n",
      "\t Val. Loss: 0.483 |  Val. Acc: 85.51%\n",
      "Epoch: 03 | Epoch Time: 1m 21s\n",
      "\tTrain Loss: 0.350 | Train Acc: 89.20%\n",
      "\t Val. Loss: 0.281 |  Val. Acc: 91.40%\n",
      "Epoch: 04 | Epoch Time: 1m 20s\n",
      "\tTrain Loss: 0.202 | Train Acc: 93.73%\n",
      "\t Val. Loss: 0.220 |  Val. Acc: 93.16%\n",
      "Epoch: 05 | Epoch Time: 1m 24s\n",
      "\tTrain Loss: 0.146 | Train Acc: 95.46%\n",
      "\t Val. Loss: 0.198 |  Val. Acc: 93.61%\n",
      "Epoch: 06 | Epoch Time: 1m 23s\n",
      "\tTrain Loss: 0.119 | Train Acc: 96.28%\n",
      "\t Val. Loss: 0.191 |  Val. Acc: 93.87%\n",
      "Epoch: 07 | Epoch Time: 1m 24s\n",
      "\tTrain Loss: 0.100 | Train Acc: 96.86%\n",
      "\t Val. Loss: 0.188 |  Val. Acc: 93.96%\n",
      "Epoch: 08 | Epoch Time: 1m 19s\n",
      "\tTrain Loss: 0.089 | Train Acc: 97.15%\n",
      "\t Val. Loss: 0.185 |  Val. Acc: 94.07%\n",
      "Epoch: 09 | Epoch Time: 1m 20s\n",
      "\tTrain Loss: 0.079 | Train Acc: 97.48%\n",
      "\t Val. Loss: 0.194 |  Val. Acc: 93.90%\n",
      "Epoch: 10 | Epoch Time: 1m 16s\n",
      "\tTrain Loss: 0.071 | Train Acc: 97.68%\n",
      "\t Val. Loss: 0.182 |  Val. Acc: 94.14%\n",
      "Test Loss: 0.167 |  Test Acc: 94.26%\n"
     ]
    }
   ],
   "source": [
    "# arabic\n",
    "run('train','ar','POSTagger_arabic')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5f37b5d2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model in train mode with lang: af\n",
      "data\\af\n",
      "Unique tokens in TEXT vocabulary: 2235\n",
      "Unique tokens in UD_TAG vocabulary: 18\n",
      "\n",
      "Number of training examples: 1315\n",
      "Number of validation examples: 194\n",
      "Number of tokens in the training set: 33894\n",
      "Number of testing examples: 425\n",
      "Tag\t\tCount\t\tPercentage\n",
      "\n",
      "NOUN\t\t7335\t\t21.6%\n",
      "ADP\t\t4365\t\t12.9%\n",
      "DET\t\t3769\t\t11.1%\n",
      "PUNCT\t\t3129\t\t 9.2%\n",
      "VERB\t\t2957\t\t 8.7%\n",
      "PRON\t\t2495\t\t 7.4%\n",
      "AUX\t\t2276\t\t 6.7%\n",
      "ADJ\t\t2168\t\t 6.4%\n",
      "CCONJ\t\t1327\t\t 3.9%\n",
      "ADV\t\t1295\t\t 3.8%\n",
      "PART\t\t926\t\t 2.7%\n",
      "SCONJ\t\t716\t\t 2.1%\n",
      "PROPN\t\t359\t\t 1.1%\n",
      "SYM\t\t323\t\t 1.0%\n",
      "X\t\t291\t\t 0.9%\n",
      "NUM\t\t163\t\t 0.5%\n",
      "The model has 858,910 trainable parameters\n",
      "Epoch: 01 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 2.582 | Train Acc: 18.24%\n",
      "\t Val. Loss: 2.415 |  Val. Acc: 21.27%\n",
      "Epoch: 02 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 2.356 | Train Acc: 22.84%\n",
      "\t Val. Loss: 2.279 |  Val. Acc: 25.92%\n",
      "Epoch: 03 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 2.148 | Train Acc: 31.57%\n",
      "\t Val. Loss: 1.987 |  Val. Acc: 38.76%\n",
      "Epoch: 04 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.767 | Train Acc: 45.34%\n",
      "\t Val. Loss: 1.517 |  Val. Acc: 55.33%\n",
      "Epoch: 05 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 1.286 | Train Acc: 62.40%\n",
      "\t Val. Loss: 1.053 |  Val. Acc: 70.13%\n",
      "Epoch: 06 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.911 | Train Acc: 73.64%\n",
      "\t Val. Loss: 0.771 |  Val. Acc: 77.69%\n",
      "Epoch: 07 | Epoch Time: 0m 7s\n",
      "\tTrain Loss: 0.670 | Train Acc: 80.90%\n",
      "\t Val. Loss: 0.598 |  Val. Acc: 83.19%\n",
      "Epoch: 08 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.515 | Train Acc: 86.00%\n",
      "\t Val. Loss: 0.492 |  Val. Acc: 86.23%\n",
      "Epoch: 09 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.403 | Train Acc: 88.99%\n",
      "\t Val. Loss: 0.426 |  Val. Acc: 87.96%\n",
      "Epoch: 10 | Epoch Time: 0m 8s\n",
      "\tTrain Loss: 0.331 | Train Acc: 90.86%\n",
      "\t Val. Loss: 0.372 |  Val. Acc: 89.71%\n",
      "Test Loss: 0.379 |  Test Acc: 89.27%\n"
     ]
    }
   ],
   "source": [
    "# afrikaans\n",
    "run('train','af','POSTagger_afrikaans')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "228e0b5c",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model in train mode with lang: lt\n",
      "data\\lt\n",
      "Unique tokens in TEXT vocabulary: 4392\n",
      "Unique tokens in UD_TAG vocabulary: 19\n",
      "\n",
      "Number of training examples: 2341\n",
      "Number of validation examples: 617\n",
      "Number of tokens in the training set: 47605\n",
      "Number of testing examples: 684\n",
      "Tag\t\tCount\t\tPercentage\n",
      "\n",
      "NOUN\t\t14933\t\t31.4%\n",
      "PUNCT\t\t8756\t\t18.4%\n",
      "VERB\t\t6604\t\t13.9%\n",
      "ADJ\t\t3274\t\t 6.9%\n",
      "CCONJ\t\t2136\t\t 4.5%\n",
      "ADV\t\t1826\t\t 3.8%\n",
      "PRON\t\t1688\t\t 3.5%\n",
      "ADP\t\t1490\t\t 3.1%\n",
      "NUM\t\t1312\t\t 2.8%\n",
      "DET\t\t1181\t\t 2.5%\n",
      "X\t\t1066\t\t 2.2%\n",
      "PROPN\t\t983\t\t 2.1%\n",
      "PART\t\t951\t\t 2.0%\n",
      "SCONJ\t\t917\t\t 1.9%\n",
      "AUX\t\t453\t\t 1.0%\n",
      "SYM\t\t26\t\t 0.1%\n",
      "INTJ\t\t9\t\t 0.0%\n",
      "The model has 1,074,867 trainable parameters\n",
      "Epoch: 01 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 2.322 | Train Acc: 29.66%\n",
      "\t Val. Loss: 2.152 |  Val. Acc: 30.69%\n",
      "Epoch: 02 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 2.067 | Train Acc: 38.39%\n",
      "\t Val. Loss: 1.923 |  Val. Acc: 43.26%\n",
      "Epoch: 03 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 1.685 | Train Acc: 48.82%\n",
      "\t Val. Loss: 1.487 |  Val. Acc: 53.54%\n",
      "Epoch: 04 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 1.289 | Train Acc: 58.86%\n",
      "\t Val. Loss: 1.205 |  Val. Acc: 61.48%\n",
      "Epoch: 05 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.970 | Train Acc: 68.27%\n",
      "\t Val. Loss: 0.975 |  Val. Acc: 68.18%\n",
      "Epoch: 06 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.734 | Train Acc: 76.34%\n",
      "\t Val. Loss: 0.826 |  Val. Acc: 73.61%\n",
      "Epoch: 07 | Epoch Time: 0m 17s\n",
      "\tTrain Loss: 0.574 | Train Acc: 81.63%\n",
      "\t Val. Loss: 0.721 |  Val. Acc: 77.15%\n",
      "Epoch: 08 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.467 | Train Acc: 85.27%\n",
      "\t Val. Loss: 0.674 |  Val. Acc: 78.77%\n",
      "Epoch: 09 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.397 | Train Acc: 87.31%\n",
      "\t Val. Loss: 0.622 |  Val. Acc: 80.15%\n",
      "Epoch: 10 | Epoch Time: 0m 16s\n",
      "\tTrain Loss: 0.347 | Train Acc: 88.88%\n",
      "\t Val. Loss: 0.609 |  Val. Acc: 80.48%\n",
      "Test Loss: 0.743 |  Test Acc: 77.16%\n"
     ]
    }
   ],
   "source": [
    "# lithuanian\n",
    "run('train','lt','POSTagger_lithuanian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "19605f62",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model in train mode with lang: hy\n",
      "data\\hy\n",
      "Unique tokens in TEXT vocabulary: 3800\n",
      "Unique tokens in UD_TAG vocabulary: 19\n",
      "\n",
      "Number of training examples: 1975\n",
      "Number of validation examples: 249\n",
      "Number of tokens in the training set: 42105\n",
      "Number of testing examples: 278\n",
      "Tag\t\tCount\t\tPercentage\n",
      "\n",
      "NOUN\t\t10524\t\t25.0%\n",
      "PUNCT\t\t8124\t\t19.3%\n",
      "VERB\t\t5355\t\t12.7%\n",
      "ADJ\t\t3317\t\t 7.9%\n",
      "AUX\t\t2963\t\t 7.0%\n",
      "CCONJ\t\t1905\t\t 4.5%\n",
      "ADV\t\t1843\t\t 4.4%\n",
      "PRON\t\t1636\t\t 3.9%\n",
      "DET\t\t1609\t\t 3.8%\n",
      "PROPN\t\t1499\t\t 3.6%\n",
      "ADP\t\t1301\t\t 3.1%\n",
      "SCONJ\t\t745\t\t 1.8%\n",
      "NUM\t\t540\t\t 1.3%\n",
      "PART\t\t458\t\t 1.1%\n",
      "X\t\t153\t\t 0.4%\n",
      "INTJ\t\t105\t\t 0.2%\n",
      "SYM\t\t28\t\t 0.1%\n",
      "The model has 1,015,667 trainable parameters\n",
      "Epoch: 01 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 2.432 | Train Acc: 23.22%\n",
      "\t Val. Loss: 2.213 |  Val. Acc: 31.20%\n",
      "Epoch: 02 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 2.204 | Train Acc: 32.36%\n",
      "\t Val. Loss: 2.012 |  Val. Acc: 39.86%\n",
      "Epoch: 03 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 1.904 | Train Acc: 41.05%\n",
      "\t Val. Loss: 1.544 |  Val. Acc: 51.58%\n",
      "Epoch: 04 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 1.461 | Train Acc: 53.67%\n",
      "\t Val. Loss: 1.170 |  Val. Acc: 62.57%\n",
      "Epoch: 05 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 1.124 | Train Acc: 63.56%\n",
      "\t Val. Loss: 0.922 |  Val. Acc: 69.94%\n",
      "Epoch: 06 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 0.888 | Train Acc: 71.36%\n",
      "\t Val. Loss: 0.761 |  Val. Acc: 75.61%\n",
      "Epoch: 07 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 0.712 | Train Acc: 77.49%\n",
      "\t Val. Loss: 0.637 |  Val. Acc: 80.09%\n",
      "Epoch: 08 | Epoch Time: 0m 14s\n",
      "\tTrain Loss: 0.575 | Train Acc: 82.29%\n",
      "\t Val. Loss: 0.558 |  Val. Acc: 83.08%\n",
      "Epoch: 09 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.482 | Train Acc: 85.16%\n",
      "\t Val. Loss: 0.510 |  Val. Acc: 84.16%\n",
      "Epoch: 10 | Epoch Time: 0m 13s\n",
      "\tTrain Loss: 0.420 | Train Acc: 87.12%\n",
      "\t Val. Loss: 0.481 |  Val. Acc: 85.18%\n",
      "Test Loss: 0.563 |  Test Acc: 81.59%\n"
     ]
    }
   ],
   "source": [
    "# armenian\n",
    "run('train','hy','POSTagger_armenian')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "946d76e6",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running model in train mode with lang: ta\n",
      "data\\ta\n",
      "Unique tokens in TEXT vocabulary: 926\n",
      "Unique tokens in UD_TAG vocabulary: 15\n",
      "\n",
      "Number of training examples: 400\n",
      "Number of validation examples: 80\n",
      "Number of tokens in the training set: 6329\n",
      "Number of testing examples: 120\n",
      "Tag\t\tCount\t\tPercentage\n",
      "\n",
      "NOUN\t\t1860\t\t29.4%\n",
      "PROPN\t\t936\t\t14.8%\n",
      "VERB\t\t747\t\t11.8%\n",
      "PUNCT\t\t665\t\t10.5%\n",
      "ADJ\t\t466\t\t 7.4%\n",
      "AUX\t\t423\t\t 6.7%\n",
      "PART\t\t383\t\t 6.1%\n",
      "ADV\t\t251\t\t 4.0%\n",
      "ADP\t\t184\t\t 2.9%\n",
      "NUM\t\t156\t\t 2.5%\n",
      "PRON\t\t147\t\t 2.3%\n",
      "DET\t\t80\t\t 1.3%\n",
      "CCONJ\t\t31\t\t 0.5%\n",
      "The model has 727,239 trainable parameters\n",
      "Epoch: 01 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 2.505 | Train Acc: 23.26%\n",
      "\t Val. Loss: 2.242 |  Val. Acc: 29.06%\n",
      "Epoch: 02 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 2.224 | Train Acc: 29.42%\n",
      "\t Val. Loss: 2.158 |  Val. Acc: 29.06%\n",
      "Epoch: 03 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 2.179 | Train Acc: 28.44%\n",
      "\t Val. Loss: 2.087 |  Val. Acc: 29.30%\n",
      "Epoch: 04 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 2.114 | Train Acc: 30.56%\n",
      "\t Val. Loss: 2.050 |  Val. Acc: 29.14%\n",
      "Epoch: 05 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 2.064 | Train Acc: 30.84%\n",
      "\t Val. Loss: 2.000 |  Val. Acc: 34.60%\n",
      "Epoch: 06 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 2.009 | Train Acc: 33.54%\n",
      "\t Val. Loss: 1.978 |  Val. Acc: 36.58%\n",
      "Epoch: 07 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 2.015 | Train Acc: 35.53%\n",
      "\t Val. Loss: 1.936 |  Val. Acc: 38.24%\n",
      "Epoch: 08 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 1.941 | Train Acc: 38.65%\n",
      "\t Val. Loss: 1.859 |  Val. Acc: 40.30%\n",
      "Epoch: 09 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 1.854 | Train Acc: 40.10%\n",
      "\t Val. Loss: 1.810 |  Val. Acc: 41.25%\n",
      "Epoch: 10 | Epoch Time: 0m 1s\n",
      "\tTrain Loss: 1.809 | Train Acc: 42.08%\n",
      "\t Val. Loss: 1.726 |  Val. Acc: 43.07%\n",
      "Test Loss: 1.821 |  Test Acc: 42.66%\n"
     ]
    }
   ],
   "source": [
    "# tamil\n",
    "run('train','ta','POSTagger_tamil')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\t Hello world\n",
      "1\t Hello world\n",
      "2\t Hello world\n",
      "3\t Hello world\n",
      "4\t Hello world\n",
      "\n",
      "   0   Hello world\n",
      "0  1   Hello world\n",
      "1  2   Hello world\n",
      "2  3   Hello world\n",
      "3  4   Hello world\n"
     ]
    }
   ],
   "source": [
    "for index in range(5):\n",
    "    with open('test.csv','a',encoding='utf-8') as f:\n",
    "        f.write(f'{index}\\t Hello world\\n')\n",
    "        f.close()\n",
    "\n",
    "with open('test.csv','r') as f:\n",
    "    print(f.read())\n",
    "\n",
    "import pandas as pd\n",
    "print(pd.read_csv('test.csv',delimiter='\\t',encoding='utf-8'))\n",
    "\n",
    "import os\n",
    "if os.path.exists(\"test.csv\"):\n",
    "  os.remove(\"test.csv\")\n",
    "else:\n",
    "  print(\"The file does not exist\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "bc666bb8",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Task 2 - Discussion (10 points)\n",
    "In this task we will discuss the received results from your evaluation. \n",
    "Each question has an additional markdown cell below for the answer. Please use it and put in you answer there."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d293d04",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Question 1: How the performance changes accross language families and available dataset size? Make a conclusion of how the model's prediction depends on the available data. (2 point)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e11c20c",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Q1 answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6bad4ba",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Question 2: What role does the training set size plays for the model? Which problem regarding training sets occurs when you deal with the low-resource languages? (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f353a4b",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Q2 answer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6ec7e44",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Question 3: What do the parameters \"n_layers\", \"bidirectional\" and \"dropout\" (variable \"params\" in the first code cell) of the LSTM model mean? According to your research results please answer the following questions regarding the low-resource languages: (4 points)\n",
    "#### - What happens when you increase the variable n_layers and why? \n",
    "#### - What changes when the model is unidirectional and why? \n",
    "#### - What happens when you increase the dropout and why?"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "![sdf](C:\\Repository\\Informatik_Local\\Ethics_In_NLP\\HW03\\img\\n_layers.png)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "id": "899f2eba",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Question 4: Define the term \"label noise\". After that, please answer what happens if the labels in the training data are noisy? (2 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41f495d",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Q4 answer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}