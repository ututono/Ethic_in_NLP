{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "hw00.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# 1. XOR function"
      ],
      "metadata": {
        "id": "qRyjQ1mjC8F4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "path = \"/content/drive/My Drive\"\n",
        "\n",
        "os.chdir(path)\n",
        "os.listdir(path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ZBMWTdlWWYQ",
        "outputId": "526f2c7c-8d41-4828-8bb4-071b82c210ea"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['negative-words.txt',\n",
              " 'positive-words.txt',\n",
              " 'Europass',\n",
              " '无标题文档.gdoc',\n",
              " 'task4-tweets20-en - task4-tweets20-en（副本）.gsheet',\n",
              " 'task4-tweets20-en - task4-tweets20-en.gsheet',\n",
              " 'task4-tweets26-5_Answer.gsheet',\n",
              " 'test.csv',\n",
              " 'train.csv',\n",
              " 'train.gsheet',\n",
              " 'Colab Notebooks',\n",
              " 'train_translated.gsheet',\n",
              " '无标题电子表格.gsheet',\n",
              " 'Anniversary.gsheet']"
            ]
          },
          "metadata": {},
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "C6jEVCKpiQ0i"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "7xckiWR-vttW"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUbYRhnwzNty"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.datasets import make_classification\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.model_selection import train_test_split\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prepare training data\n",
        "X=[\n",
        "   [0,0],\n",
        "   [0,1],\n",
        "   [1,0],\n",
        "   [1,1]\n",
        "]\n",
        "y=[-1,1,1,-1]\n",
        "\n",
        "# convert X,y to np array\n",
        "X=np.array(X)\n",
        "y=np.array(y)"
      ],
      "metadata": {
        "id": "UdizQpIB6umD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# define neural network\n",
        "mlp=MLPClassifier(random_state=1,max_iter=100000, hidden_layer_sizes=(5,5),\n",
        "                 activation='relu', alpha=0.0001, \n",
        "                 solver='lbfgs',verbose=True)\n",
        "# set output node to use sigmoid activation function\n",
        "mlp.out_activation_='logistic'"
      ],
      "metadata": {
        "id": "8a3l_QxW7MLi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model\n",
        "trained_model=mlp.fit(X,y)\n"
      ],
      "metadata": {
        "id": "vrxceNmp929K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test the model\n",
        "test=[[0,1],[0,0],[1,1],[1,0]]\n",
        "trained_model.predict(np.array(test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Qz4vsanm-BpB",
        "outputId": "b8b57df9-c9c8-48dc-ec60-0f83e3635383"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-1, -1, -1,  1])"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# weights\n",
        "for w in trained_model.coefs_:\n",
        "  print(w.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QEnGzwR0_t4P",
        "outputId": "9f522ac7-d4f4-4766-c545-fd247d19ef27"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 5)\n",
            "(5, 5)\n",
            "(5, 1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# bias\n",
        "trained_model.intercepts_"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0TiI3Y40CHBV",
        "outputId": "48850884-768e-4756-fa39-76c8ec34bb8e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[array([-0.14962269, -2.97043085, -0.5472481 ,  1.36215335, -0.87510813]),\n",
              " array([ 1.88523531, -0.82865212, -0.80904768, -0.03082989, -0.76999685]),\n",
              " array([-7.0403055])]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2 Text classifcation"
      ],
      "metadata": {
        "id": "cCFRIpYHDMxV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 2.1 Sentiment lexicon-based classifier"
      ],
      "metadata": {
        "id": "cSYl0e06F45n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "dxs5JbaLXTRB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "import nltk # just use a corpus named stopwords to optimize built-in tokenizer\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "from nltk.stem.porter import *"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hYoUHlwOoJoR",
        "outputId": "bb25ab11-cf16-4c09-c17d-fd01b67edefe"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def reader():\n",
        "  root_dir='Ethic_in_NLP/DATA/review_polarity/txt_sentoken'\n",
        "\n",
        "  neg_reviews=read(root_dir,'neg')\n",
        "  pos_reviews=read(root_dir,'pos')\n",
        "  return neg_reviews,pos_reviews\n",
        "\n",
        "\n",
        "def read(path, category):\n",
        "  \"\"\"\n",
        "  \"\"\"\n",
        "  reviews=[]\n",
        "  root_dir=path+'/'+category\n",
        "  for file in os.listdir(root_dir):\n",
        "    file_name=root_dir+'/'+file\n",
        "    file_stream=open(file_name,'r')\n",
        "    review=''\n",
        "    for line in file_stream:\n",
        "      line=line.strip('\\n')\n",
        "      review+=line\n",
        "    text_noPunc=re.sub(\"[^a-zA-Z#]\",\" \", review)\n",
        "    words=text_noPunc.lower().split() # cast word to lower and split them by space\n",
        "    reviews.append(words)\n",
        "    file_stream.close()\n",
        "  \n",
        "  return reviews\n",
        "  \n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "6x1cpXxSDUaq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "520fb2d8-097e-407c-d9fb-dde8a5bd4d26"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 1000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Lexicon_Classifier():\n",
        "  def __init__(self) -> None:\n",
        "    self.posDict,self.negDict=self.loadData()\n",
        "\n",
        "  \n",
        "  def loadData(self):\n",
        "    pos_path='positive-words.txt'\n",
        "    neg_path='negative-words.txt'\n",
        "\n",
        "\n",
        "    poswords=[]\n",
        "    negwords=[]\n",
        "    with open(file=pos_path) as f:\n",
        "      for line in f:\n",
        "        if not line.startswith(';'):\n",
        "          poswords.append(line)\n",
        "    \n",
        "    with open(file=neg_path) as f:\n",
        "      for line in f:\n",
        "        if not line.startswith(';'):\n",
        "          negwords.append(line)\n",
        "\n",
        "\n",
        "\n",
        "    pos_dict=defaultdict(int)\n",
        "    neg_dict=defaultdict(int)\n",
        "    for item in poswords:\n",
        "      item=item.strip('\\n')\n",
        "      if not item.isspace():\n",
        "        pos_dict[item]+=1\n",
        "    for item in negwords:\n",
        "      item=item.strip('\\n')\n",
        "      if not item.isspace():\n",
        "        neg_dict[item]+=1\n",
        "\n",
        "    return pos_dict,neg_dict\n",
        "\n",
        "  \n",
        "  def predict(self, X):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    predicts=[]\n",
        "    for item in X:\n",
        "      pred_label=self.classify(item)\n",
        "      predicts.append(pred_label)\n",
        "    \n",
        "    return predicts\n",
        "\n",
        "\n",
        "  def classify(self, sent):\n",
        "    \"\"\"\n",
        "    :sent: a tokenized sentence from review\n",
        "    \"\"\"\n",
        "    # number of postive words, negative words\n",
        "    pos_num=0\n",
        "    neg_num=0\n",
        "    for word in sent:\n",
        "      if self.posDict.get(word)!=None:\n",
        "        pos_num+=1\n",
        "      if self.negDict.get(word)!=None:\n",
        "        neg_num+=1\n",
        "    \n",
        "    # print(pos_num,neg_num)\n",
        "    if pos_num>neg_num:\n",
        "      return 1\n",
        "    elif pos_num<neg_num:\n",
        "      return -1\n",
        "    else:\n",
        "      return 0\n",
        "\n",
        "  def evaluate(self, pred_y, gold_y):\n",
        "    f1=f1_score(gold_y,pred_y,average='macro')\n",
        "    ac=accuracy_score(gold_y, pred_y)\n",
        "\n",
        "    print(\"The accuracy is {0}. The f1-score(average='macro') is {1}\".format(ac,f1))\n",
        "\n",
        "\n",
        "\n",
        "    \n",
        "    \n",
        "\n",
        "  def main_task_1_2_1():  \n",
        "    neg_reviews,pos_reviews=reader()\n",
        "    lc=Lexicon_Classifier()\n",
        "\n",
        "    predicts=lc.predict(neg_reviews)\n",
        "\n",
        "    gold_set_np=np.ones(shape=(len(neg_reviews),1))*(-1)\n",
        "    gold=gold_set_np.tolist()\n",
        "    lc.evaluate(predicts,gold)\n",
        "\n",
        "\n",
        "\n",
        "main_task_1_2_1()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Kp6aSwH4F_53",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "735900db-1a88-4a41-fb8f-7b057b643f88"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The accuracy is 0.719. The f1-score(average='macro') is 0.2788442893154935\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YgUelD_S6_8-"
      }
    }
  ]
}