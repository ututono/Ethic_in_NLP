{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "hw00.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# 1. XOR function"
   ],
   "metadata": {
    "id": "qRyjQ1mjC8F4"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import os"
   ],
   "metadata": {
    "id": "C6jEVCKpiQ0i"
   },
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import accuracy_score"
   ],
   "metadata": {
    "id": "7xckiWR-vttW"
   },
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "NUbYRhnwzNty"
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# prepare training data\n",
    "X=[\n",
    "   [0,0],\n",
    "   [0,1],\n",
    "   [1,0],\n",
    "   [1,1]\n",
    "]\n",
    "y=[-1,1,1,-1]\n",
    "\n",
    "# convert X,y to np array\n",
    "X=np.array(X)\n",
    "y=np.array(y)"
   ],
   "metadata": {
    "id": "UdizQpIB6umD"
   },
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# define neural network\n",
    "mlp=MLPClassifier(random_state=1,max_iter=100000, hidden_layer_sizes=(5,5),\n",
    "                 activation='relu', alpha=0.0001, \n",
    "                 solver='lbfgs',verbose=True)\n",
    "# set output node to use sigmoid activation function\n",
    "mlp.out_activation_='logistic'"
   ],
   "metadata": {
    "id": "8a3l_QxW7MLi"
   },
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Train the model\n",
    "trained_model=mlp.fit(X,y)\n"
   ],
   "metadata": {
    "id": "vrxceNmp929K"
   },
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "# Test the model\n",
    "test=[[0,1],[0,0],[1,1],[1,0]]\n",
    "trained_model.predict(np.array(test))"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Qz4vsanm-BpB",
    "outputId": "165b5bd4-a482-4e2f-f12a-094b55ee079c"
   },
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "array([-1, -1, -1,  1])"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# weights\n",
    "for w in trained_model.coefs_:\n",
    "  print(w.shape)"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QEnGzwR0_t4P",
    "outputId": "e004a722-2068-4a75-837b-2af00c77ef20"
   },
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 5)\n",
      "(5, 5)\n",
      "(5, 1)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# bias\n",
    "trained_model.intercepts_"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0TiI3Y40CHBV",
    "outputId": "e76157db-f94e-4390-b78f-b1e2ebbaccc8"
   },
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "[array([-0.14962269, -1.54165669, -0.5472481 ,  1.69767268, -0.87510813]),\n array([ 3.58527483, -0.52189389, -0.80904768, -0.03082989, -0.76999685]),\n array([-13.17581923])]"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# 2 Text classifcation"
   ],
   "metadata": {
    "id": "cCFRIpYHDMxV"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.1 Sentiment lexicon-based classifier"
   ],
   "metadata": {
    "id": "cSYl0e06F45n"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "from collections import defaultdict\n",
    "import re\n",
    "import nltk # just use a corpus named stopwords to optimize built-in tokenizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "from nltk.stem.porter import *"
   ],
   "metadata": {
    "id": "dxs5JbaLXTRB"
   },
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "\n",
    "def reader():\n",
    "  root_dir='DATA/review_polarity/txt_sentoken'\n",
    "\n",
    "  neg_reviews=read(root_dir,'neg')\n",
    "  pos_reviews=read(root_dir,'pos')\n",
    "  return neg_reviews,pos_reviews\n",
    "\n",
    "\n",
    "def read(path, category):\n",
    "  \"\"\"\n",
    "  load data from file, and combine sentences from the same file to one string without punctuation\n",
    "  \"\"\"\n",
    "  reviews=[]\n",
    "\n",
    "  # phase file path\n",
    "  root_dir=path+'/'+category\n",
    "  for file in os.listdir(root_dir):\n",
    "    file_name=root_dir+'/'+file\n",
    "    file_stream=open(file_name,'r')\n",
    "    review=''\n",
    "\n",
    "    # combine sentences\n",
    "    for line in file_stream:\n",
    "      line=line.strip('\\n')\n",
    "      review+=line\n",
    "\n",
    "    # remove punctuation\n",
    "    text_noPunc=re.sub(\"[^a-zA-Z#]\",\" \", review)\n",
    "\n",
    "    # cast alphabets to lower and split the string combining sentence\n",
    "    words=text_noPunc.lower().split() # cast word to lower and split them by space\n",
    "    reviews.append(words)\n",
    "    file_stream.close()\n",
    "  \n",
    "  return reviews\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "class Lexicon_Classifier():\n",
    "  \"\"\"\n",
    "  classifier positive and negative by comparing the number of the positive words and negative words occurred in the given lexicon.\n",
    "\n",
    "  Attribute:\n",
    "  - posDict: A dictionary consisting of the positive word from the give lexicon\n",
    "  - negDict: A dictionary consisting of the negative word from the give lexicon\n",
    "\n",
    "\n",
    "  \"\"\"\n",
    "  def __init__(self) -> None:\n",
    "    self.posDict,self.negDict=self.loadData()\n",
    "\n",
    "\n",
    "  def loadData(self):\n",
    "    pos_path='DATA/opinion_lexicon_English/positive-words.txt'\n",
    "    neg_path='DATA/opinion_lexicon_English/negative-words.txt'\n",
    "\n",
    "\n",
    "    poswords=[]\n",
    "    negwords=[]\n",
    "    with open(file=pos_path) as f:\n",
    "      for line in f:\n",
    "        if not line.startswith(';'):\n",
    "          poswords.append(line)\n",
    "\n",
    "    with open(file=neg_path) as f:\n",
    "      for line in f:\n",
    "        if not line.startswith(';'):\n",
    "          negwords.append(line)\n",
    "\n",
    "    pos_dict=defaultdict(int)\n",
    "    neg_dict=defaultdict(int)\n",
    "    for item in poswords:\n",
    "      item=item.strip('\\n')\n",
    "      if not item.isspace():\n",
    "        pos_dict[item]+=1\n",
    "    for item in negwords:\n",
    "      item=item.strip('\\n')\n",
    "      if not item.isspace():\n",
    "        neg_dict[item]+=1\n",
    "\n",
    "    return pos_dict,neg_dict\n",
    "\n",
    "\n",
    "  def predict(self, X):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    predicts=[]\n",
    "    for item in X:\n",
    "      pred_label=self.classify(item)\n",
    "      predicts.append(pred_label)\n",
    "\n",
    "    return predicts\n",
    "\n",
    "\n",
    "  def classify(self, sent):\n",
    "    \"\"\"\n",
    "    :sent: a tokenized sentence from review\n",
    "    \"\"\"\n",
    "    # number of positive words, negative words\n",
    "    pos_num=0\n",
    "    neg_num=0\n",
    "\n",
    "    # count the number of pos/neg words in a review\n",
    "    for word in sent:\n",
    "      if self.posDict.get(word)!=None:\n",
    "        pos_num+=1\n",
    "      if self.negDict.get(word)!=None:\n",
    "        neg_num+=1\n",
    "\n",
    "    # print(pos_num,neg_num)\n",
    "    if pos_num>neg_num:\n",
    "      return 1\n",
    "    elif pos_num<neg_num:\n",
    "      return -1\n",
    "    else:\n",
    "      return 0\n",
    "\n",
    "  def evaluate(self, pred_y, gold_y):\n",
    "    f1=f1_score(gold_y,pred_y,average='macro')\n",
    "    ac=accuracy_score(gold_y, pred_y)\n",
    "\n",
    "    print(\"The accuracy is {0}.\\nThe f1-score(average='macro') is {1}\".format(ac,f1))\n",
    "\n",
    "\n",
    "\n",
    "def main_task_1_2_1():\n",
    "  neg_reviews,pos_reviews=reader()\n",
    "  lc=Lexicon_Classifier()\n",
    "\n",
    "  predicts=lc.predict(neg_reviews)\n",
    "\n",
    "  # generate gold labels which actually are all -1 in this case\n",
    "  gold_set_np=np.ones(shape=(len(neg_reviews),1))*(-1)\n",
    "  gold=gold_set_np.tolist()\n",
    "  lc.evaluate(predicts,gold)\n",
    "\n",
    "\n",
    "main_task_1_2_1()"
   ],
   "metadata": {
    "id": "6x1cpXxSDUaq"
   },
   "execution_count": 76,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The accuracy is 0.719.\n",
      "The f1-score(average='macro') is 0.2788442893154935\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2.2 regressor classifier"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "from nltk.stem.porter import *\n",
    "from nltk.probability import FreqDist"
   ],
   "metadata": {
    "id": "G0JVWtsUAt89",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "8d2b7936-e58b-4ce4-bc91-c7629440d698"
   },
   "execution_count": 15,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to C:\\Users\\Marco\n",
      "[nltk_data]     Yu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\Marco\n",
      "[nltk_data]     Yu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Marco Yu\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd"
   ],
   "metadata": {
    "id": "ZfDUdehYHnPW"
   },
   "execution_count": 16,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "def reader():\n",
    "  root_dir='DATA/review_polarity/txt_sentoken'\n",
    "\n",
    "  neg_reviews=read(root_dir,'neg')\n",
    "  pos_reviews=read(root_dir,'pos')\n",
    "  return neg_reviews,pos_reviews\n",
    "\n",
    "\n",
    "def read(path, category):\n",
    "  \"\"\"\n",
    "\n",
    "\n",
    "  \"\"\"\n",
    "  # initialize a lemmatizer to lemmatize word\n",
    "  lemmatizer=WordNetLemmatizer()\n",
    "  stemmer=PorterStemmer()\n",
    "  tag=5\n",
    "  reviews=[]\n",
    "  root_dir=path+'/'+category\n",
    "  for file in os.listdir(root_dir):\n",
    "    file_name=root_dir+'/'+file\n",
    "    file_stream=open(file_name,'r')\n",
    "    review=''\n",
    "    for line in file_stream:\n",
    "      line=line.strip('\\n')\n",
    "      review+=line\n",
    "\n",
    "    # Text pre-processing \n",
    "\n",
    "\n",
    "    # remove punctation and cast all aphabets to lower one\n",
    "    text_noPunc=re.sub(\"[^a-zA-Z#]\",\" \", review)\n",
    "    words=text_noPunc.lower().split() \n",
    "    \n",
    "\n",
    "\n",
    "    # normalization\n",
    "    stop_words=stopwords.words('english')\n",
    "    words=[stemmer.stem(lemmatizer.lemmatize(word)) for word in words \n",
    "           if word not in stop_words and len(word)>2 ]\n",
    "\n",
    "    reviews.append(words)\n",
    "    file_stream.close()\n",
    "  \n",
    "  return reviews\n"
   ],
   "metadata": {
    "id": "nG24jaCiBXJk"
   },
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": [
    "%time\n",
    "\n",
    "class Vectorizer():\n",
    "  \"\"\"\n",
    "\n",
    "  \"\"\"\n",
    "\n",
    "  def __init__(self,vocabulary) -> None:\n",
    "    self.vocab=vocabulary\n",
    "\n",
    "\n",
    "  def loadData(self):\n",
    "    pos_path='DATA/opinion_lexicon_English/positive-words.txt'\n",
    "    neg_path='DATA/opinion_lexicon_English/negative-words.txt'\n",
    "\n",
    "\n",
    "    poswords=[]\n",
    "    negwords=[]\n",
    "    with open(file=pos_path) as f:\n",
    "      for line in f:\n",
    "        if not line.startswith(';'):\n",
    "          poswords.append(line)\n",
    "\n",
    "    with open(file=neg_path) as f:\n",
    "      for line in f:\n",
    "        if not line.startswith(';'):\n",
    "          negwords.append(line)\n",
    "\n",
    "    pos_dict=defaultdict(int)\n",
    "    neg_dict=defaultdict(int)\n",
    "    for item in poswords:\n",
    "      item=item.strip('\\n')\n",
    "      if not item.isspace():\n",
    "        pos_dict[item]+=1\n",
    "    for item in negwords:\n",
    "      item=item.strip('\\n')\n",
    "      if not item.isspace():\n",
    "        neg_dict[item]+=1\n",
    "\n",
    "    return pos_dict,neg_dict\n",
    "\n",
    "  \n",
    "  def vectorize(self,sentences):\n",
    "    \"\"\"\n",
    "    for 2k sampled reviews, traning data costs 7 min at least \n",
    "    on colab with the standard virtual hardware\n",
    "\n",
    "    \"\"\"\n",
    "    # initialize BOW matrix\n",
    "    vectors_df=pd.DataFrame(data=np.zeros((1,len(self.vocab))).tolist(), columns=self.vocab, dtype=int)\n",
    "\n",
    "    # count words and update BOW matrix\n",
    "    for index, review in enumerate(sentences):\n",
    "      vectors_df.loc[index]=0\n",
    "      fq=nltk.FreqDist(review)\n",
    "      for key in fq.keys():\n",
    "        vectors_df.loc[index,key]+=fq.get(key)\n",
    "\n",
    "    # reduce dimensions of the BOW matrix according word frequency\n",
    "    fq_list=[]\n",
    "    # calculate how many times the word occurs over the whole texts\n",
    "    for key in vectors_df.keys():\n",
    "      fq_list.append(vectors_df[key].sum())\n",
    "\n",
    "    # sorting word frequency\n",
    "    feature_fq=list(nltk.FreqDist(fq_list).keys())\n",
    "    feature_fq.sort()\n",
    "\n",
    "    # drop features that rarely present in texts\n",
    "    drop_list=[key for key in feature_fq[:10]]\n",
    "    keys=[]\n",
    "    for key in vectors_df.keys():\n",
    "      if vectors_df[key].sum() in drop_list:\n",
    "        keys.append(key)\n",
    "\n",
    "    vectors_df_reduced=vectors_df.drop(columns=keys)\n",
    "\n",
    "\n",
    "    self.vocab=vectors_df_reduced\n",
    "    os.makedirs('model/', exist_ok=True)\n",
    "    vectors_df_reduced.to_csv('model/BOW_Model.csv')\n",
    "    return vectors_df_reduced\n",
    "\n",
    "def main_1_2_2():\n",
    "  pos_set, neg_set = reader()\n",
    "\n",
    "  # generate word vocabulary from all given texts\n",
    "  vocab=[]\n",
    "  for cate in (pos_set, neg_set):\n",
    "    for sent in cate:\n",
    "      for word in sent:\n",
    "        vocab.append(word)\n",
    "\n",
    "  total_reviews=pos_set+neg_set\n",
    "  vc=Vectorizer(set(vocab))\n",
    "\n",
    "  # if you run the program at the first time, you should vectorize the text by running \"vc.vectorize()\"\n",
    "  # Otherwise, you just annotate this line of code, and remove the annotation symbol of the line \"pd.read_csv()\"\n",
    "  # to read trained model from the file generated after the program running the first time in order to save time\n",
    "  vv_df=vc.vectorize(total_reviews)\n",
    "  # vv_df=pd.read_csv('Ethic_in_NLP/model/BOW_Model.csv')\n",
    "\n",
    "  # generate gold labels for the reviews\n",
    "  label_pos=np.ones(shape=(len(pos_set)))\n",
    "  label_neg=np.zeros(shape=(len(neg_set)))\n",
    "  y=np.concatenate((label_pos,label_neg))\n",
    "\n",
    "  X=vv_df.to_numpy()\n",
    "\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1, test_size=0.3)\n",
    "\n",
    "  # LR Classifier\n",
    "  logistic_regressor=LogisticRegression(random_state=0,max_iter=1000,C=0.01)\n",
    "\n",
    "  # training data\n",
    "  trained_model=logistic_regressor.fit(X_train, y_train)\n",
    "\n",
    "  # predict for test data\n",
    "  predictions=trained_model.predict(X_test)\n",
    "\n",
    "  # evaluation\n",
    "  print(classification_report(y_test,predictions))\n",
    "\n",
    "main_1_2_2()  # Word vectorization need almost 5 minutes.\n"
   ],
   "metadata": {
    "id": "ZIEbn7HXeYDn",
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "execution_count": 96,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 0 ns\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.82      0.82      0.82       301\n",
      "         1.0       0.82      0.82      0.82       299\n",
      "\n",
      "    accuracy                           0.82       600\n",
      "   macro avg       0.82      0.82      0.82       600\n",
      "weighted avg       0.82      0.82      0.82       600\n",
      "\n"
     ]
    }
   ]
  }
 ]
}