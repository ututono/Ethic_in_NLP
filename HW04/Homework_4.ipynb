{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wjyW3pQzqUYD"
      },
      "source": [
        "# Ethics for NLP: Spring 2022\n",
        "# Homework 4 Privacy\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbK8L5kJJyvX"
      },
      "source": [
        "## 1. Data Overview and Baseline\n",
        "\n",
        "A major problem with utilizing web data as a source for NLP applications is the increasing concern for privacy, e.g., such as microtargeting. This homework is aimed at developing a method to obfuscate demographic features, in this case (binary) gender and to investigate the trade-off between obfuscating an users identity and preserving useful information.\n",
        "\n",
        "The given dataset consists of Reddit posts (`post_text`) which are annotated with the gender (`op_gender`) of the user and the corresponding subreddit (`subreddit`) category.\n",
        "\n",
        "*  `subreddit_classifier.pickle` pretrained subreddit classifier\n",
        "*  `gender_classifier.pickle` pretrained gender classifier\n",
        "*  `test.csv` your primary test data\n",
        "*  `male.txt` a list of words commonly used by men\n",
        "*  `female.txt` a list of words commonly used by women\n",
        "*  `background.csv` additional Reddit posts that you may optionally use for training an obfuscation model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AvClU2_3dpsp"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import accuracy_score\n",
        "from pandas.core.frame import DataFrame\n",
        "from typing import List, Tuple\n",
        "import pandas\n",
        "import pickle\n",
        "import random\n",
        "import nltk\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5oiaDj5QNq9"
      },
      "outputs": [],
      "source": [
        "def get_preds(cache_name: str, test: List[str]) -> List[str]:\n",
        "    loaded_model, dictionary, transpose, train_bow = pickle.load(open(cache_name, 'rb'))\n",
        "    X_test = transpose(test, train_bow, dictionary)\n",
        "    preds = loaded_model.predict(X_test)\n",
        "    return preds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KoT-ItJ40d1k"
      },
      "outputs": [],
      "source": [
        "def run_classifier(test_file: str) -> Tuple[float]:\n",
        "    test_data = pandas.read_csv(test_file)\n",
        "\n",
        "    cache_name = 'gender_classifier.pickle'\n",
        "    test_preds = get_preds(cache_name, list(test_data[\"post_text\"]))\n",
        "    gold_test = list(test_data[\"op_gender\"])\n",
        "    gender_acc = accuracy_score(list(test_preds), gold_test)\n",
        "    print(\"Gender classification accuracy\", gender_acc)\n",
        "\n",
        "    cache_name = 'subreddit_classifier.pickle'\n",
        "    test_preds = get_preds(cache_name, list(test_data[\"post_text\"]))\n",
        "    gold_test = list(test_data[\"subreddit\"])\n",
        "    subreddit_acc = accuracy_score(list(test_preds), gold_test)\n",
        "    print(\"Subreddit classification accuracy\", subreddit_acc)\n",
        "    return gender_acc, subreddit_acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bf7nYEb0QPtU"
      },
      "outputs": [],
      "source": [
        "gender_acc, subreddit_acc = run_classifier(\"test.csv\")\n",
        "\n",
        "assert gender_acc == 0.646\n",
        "assert subreddit_acc == 0.832"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9LKaoI6TZJpt"
      },
      "source": [
        "**Default accuracy:**\n",
        "*   `Gender    classification accuracy: 0.646`\n",
        "*   `Subreddit classification accuracy: 0.832`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Zl4YplkJgcQ"
      },
      "source": [
        "## 2. Obfuscation of the Test Dataset\n",
        "### 2.1 Random Obfuscated Dataset  (4P)\n",
        "First, run a random experiment, by randomly swapping gender-specific words that appear in posts with a word from the respective list of words of the opposite gender.\n",
        "\n",
        "*  Write a function to read the female.txt and male.txt files\n",
        "*  Tokenize the posts („post_text“) using NLTK (0.5p)\n",
        "*  For each post, if written by a man („M“) and containing a token from the male.txt, replace that token with a random one from the female.txt (1p)\n",
        "*  For each post, if written by a woman („W“) and containing a token from the female.txt, replace that token with a random one from the male.txt (1p)\n",
        "*  Save the obfuscated version of the test.csv in a separate csv file (using pandas and makes sure to name them accordingly) (0.5p)\n",
        "*  Run the given classifier again, report the accuracy and provide a brief commentary on the results compared to the baseline (1p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8g4VhvJ7uXD"
      },
      "outputs": [],
      "source": [
        "def read_data(file_name: str) -> List[str]:\n",
        "    \"\"\"\n",
        "    \n",
        "    add your code here\n",
        "\n",
        "    \"\"\"\n",
        "    return []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UAcIf9Ck521o"
      },
      "outputs": [],
      "source": [
        "male_words = read_data(\"add/your/path\")\n",
        "female_words = read_data(\"add/your/path\")\n",
        "\n",
        "assert len(male_words) == 3000\n",
        "assert len(male_words) == 3000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wB1a1TuP8A01"
      },
      "outputs": [],
      "source": [
        "def obfuscate_gender_randomly(male_words: List[str], female_words: List[str], dataset_file_name: str) -> DataFrame:\n",
        "  \"\"\"\n",
        "  \n",
        "  add your code here\n",
        "  \n",
        "  \"\"\"\n",
        "  return DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TzXQAIxJg9Hn"
      },
      "outputs": [],
      "source": [
        "file_name = \"add file name\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QJLnNKpwwtqo"
      },
      "outputs": [],
      "source": [
        "random_replaced_test = obfuscate_gender_randomly(male_words=male_words, female_words=female_words, dataset_file_name=\"test.csv\")\n",
        "random_replaced_test.to_csv(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xNiDfkgPm9DY"
      },
      "outputs": [],
      "source": [
        "random_replaced_test = pandas.read_csv(file_name)\n",
        "assert len(random_replaced_test) == 500\n",
        "assert random_replaced_test[\"subreddit\"][0] == \"funny\"\n",
        "assert random_replaced_test[\"subreddit\"][-1:].item() == \"relationships\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "osyy2rw9J8X1"
      },
      "outputs": [],
      "source": [
        "gender_acc, subreddit_acc = run_classifier(file_name)\n",
        "\n",
        "assert gender_acc <= 0.5\n",
        "assert subreddit_acc >= 0.7"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S9AsCLRtYXc8"
      },
      "source": [
        "**Report accuracy:**\n",
        "*   `Gender    classification accuracy: `\n",
        "*   `Subreddit classification accuracy: `\n",
        "*   `Your commentary: ` ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4OHsp4B7Jqnv"
      },
      "source": [
        "### 2.2 Similarity Obfuscated Dataset (4P)\n",
        "In a second approach, refine the swap method. Instead of randomly selecting a word, use a similarity metric.\n",
        "\n",
        "\n",
        "*  Instead of the first method replace the tokens by semantically similar tokens from the other genders token list. For that you may choose any metric for identifying semantically similar words, but you have to justify your choice. (Recommend: using cosine distance between pre-trained word embeddings) (2p)\n",
        "*  Save the obfuscated version of the test.csv in a separate CSV file (using pandas and makes sure to name them accordingly) (0.5p)\n",
        "*  Run the given classifier again, report the accuracy and provide a brief commentary on the results (compared to the baseline and your other results) (1p)\n",
        "*  The classifiers accuracy for predicting the gender should be below random guessing (50%) and for the subreddit prediction it should be above 80% (0.5p)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mDblihF2fkOh"
      },
      "outputs": [],
      "source": [
        "def obfuscate_gender_by_similarity(male_words: List[str], female_words: List[str], dataset_file_name: str) -> DataFrame:\n",
        "  \"\"\"\n",
        "  \n",
        "  add your code here\n",
        "  \n",
        "  \"\"\"\n",
        "  return DataFrame()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kYAS5eXqcEoe"
      },
      "outputs": [],
      "source": [
        "\"\"\"\n",
        " you may use gensim models for example word2vec-google-news-300\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Oo1vZ0aZhHb6"
      },
      "outputs": [],
      "source": [
        "file_name = \"add file name\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1DPcGRD06UFN"
      },
      "outputs": [],
      "source": [
        "similarity_replaced_test = obfuscate_gender_by_similarity(male_words=male_words, female_words=female_words, dataset_file_name=\"./test.csv\")\n",
        "similarity_replaced_test.to_csv(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vl6yL2mTmDRX"
      },
      "outputs": [],
      "source": [
        "similarity_replaced_test = pandas.read_csv(file_name)\n",
        "assert len(similarity_replaced_test) == 500\n",
        "assert similarity_replaced_test[\"subreddit\"][0] == \"funny\"\n",
        "assert similarity_replaced_test[\"subreddit\"][-1:].item() == \"relationships\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LAgAPPSrLWsK"
      },
      "outputs": [],
      "source": [
        "gender_acc, subreddit_acc = run_classifier(file_name)\n",
        "\n",
        "assert gender_acc <= 0.5\n",
        "assert subreddit_acc >= 0.8"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gapuF5CTZCK-"
      },
      "source": [
        "**Report accuracy:**\n",
        "*   `Gender    classification accuracy: `\n",
        "*   `Subreddit classification accuracy: ` \n",
        "*   `Your commentary: ` ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mkdkD89GoxDo"
      },
      "source": [
        "### 2.3 Your Own Obfuscated Dataset (4P)\n",
        "With this last approach, you can experiment by yourself how to obfuscate the posts.\n",
        "\n",
        "*  Some examples: What if you randomly decide whether or not to replace words instead of replacing every lexicon word? What if you only replace words that have semantically similar enough counterparts? What if you use different word embeddings? (2p)\n",
        "*  Save the obfuscated version of the test.csv in a separate csv file (using pandas and makes sure to name them accordingly) (0.5p)\n",
        "*  Describe your modifications and report the accuracy and provide a brief commentary on the results compared to the baseline and your other results (1.5p)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def obfuscate_gender(dataset_file_name: str) -> DataFrame:\n",
        "  \"\"\"\n",
        "\n",
        "    add your own implemntation, you may add more functions and arguments\n",
        "    \n",
        "  \"\"\"\n",
        "  return DataFrame()"
      ],
      "metadata": {
        "id": "5mmmxiTp0SfP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HD9heixWhpHE"
      },
      "outputs": [],
      "source": [
        "file_name = \"add file name\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NnegjulAuW-k"
      },
      "outputs": [],
      "source": [
        "your_test = obfuscate_gender(dataset_file_name=\"./test.csv\")\n",
        "your_test.to_csv(file_name)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zXJVOsePuEs7"
      },
      "outputs": [],
      "source": [
        "your_test = pandas.read_csv(file_name)\n",
        "assert len(your_test) == 500\n",
        "assert your_test[\"subreddit\"][0] == \"funny\"\n",
        "assert your_test[\"subreddit\"][-1:].item() == \"relationships\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MO1tJMhzoqsL"
      },
      "outputs": [],
      "source": [
        "gender_acc, subreddit_acc = run_classifier(file_name)\n",
        "\n",
        "assert gender_acc <= 0.5\n",
        "assert subreddit_acc >= 0.6"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "US3Gcok5qdYo"
      },
      "source": [
        "**Report accuracy:**\n",
        "*   `Gender    classification accuracy: `\n",
        "*   `Subreddit classification accuracy: ` \n",
        "*   `Your commentary: ` ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "48j9qFLJFygB"
      },
      "source": [
        "### 3 Advanced Obfuscated Model (5P)\n",
        "Develop your own obfuscation model using the provided background.csv for training. Your ultimate goal should be to obfuscate text so that the classifier is unable to determine the gender of an user (no better than random guessing) without compromising the accuracy of the subreddit classification task. To train a model that is good at predicting subreddit classification, but bad at predicting gender. The key idea in this approach is to design a model that does not encode information about protected attributes (in this case, gender). In your report, include a description of your model and results.\n",
        "\n",
        "*  Develop your own classifier (3p)\n",
        "*  Use only posts from the subreddits „CasualConversation“ and „funny“ (min. 1000 posts for each gender per subreddit) (0.5p)\n",
        "*  Use sklearn models (MLPClassifier, LogisticRegression, etc.)\n",
        "*  Use 90% for training and 10% for testing (0.5p)\n",
        "*  In your report, include a description of your model and report the accuracy on the unmodified train data (your baseline here) as well as the modified train data and provide a brief commentary on the results (1p)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "\n",
        "add your code here\n",
        "\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "KrkWTZJc3uNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KoePMqHbZE9p"
      },
      "source": [
        "**Report accuracy:**\n",
        "* Baseline:\n",
        "  * `Gender    classification accuracy: `\n",
        "  * `Subreddit classification accuracy: `\n",
        "* Your Model: \n",
        "  * `Gender    classification accuracy: `\n",
        "  * `Subreddit classification accuracy: ` \n",
        "*   `Your commentary: ` ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZERIDpgwnj_"
      },
      "source": [
        "### 4 Ethical Implications (3P)\n",
        "Discuss the ethical implications of obfuscation and privacy based on the concepts covered in the lecture. Provide answers to the following points:\n",
        "\n",
        "1.   What are demographic features (name at least three) and explain shortly some of the privacy violation risks? (1p)\n",
        "2.   Explain the cultural and social implications and their effects? In this context discuss the information privacy paradox. You may refer to a recent example like the COVID-19 pandemic.  (1.5p)\n",
        "3.   Name a at least three privacy preserving countermeasures  (0.5p)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Q_Qbw0xw3W0"
      },
      "source": [
        "Your Answer: ...\n",
        "\n",
        "1. ...\n",
        "2. ...\n",
        "3. ...\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Homework_4_.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}