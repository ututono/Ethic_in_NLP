{"cells":[{"cell_type":"markdown","metadata":{"id":"wjyW3pQzqUYD"},"source":["# Ethics for NLP: Spring 2022\n","# Homework 4 Privacy\n","\n"]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7OGTZ9PxmwFG","executionInfo":{"status":"ok","timestamp":1657223068024,"user_tz":-120,"elapsed":2227,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}},"outputId":"29212d55-b06e-454c-dabd-2c063778131e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"markdown","metadata":{"id":"SbK8L5kJJyvX"},"source":["## 1. Data Overview and Baseline\n","\n","A major problem with utilizing web data as a source for NLP applications is the increasing concern for privacy, e.g., such as microtargeting. This homework is aimed at developing a method to obfuscate demographic features, in this case (binary) gender and to investigate the trade-off between obfuscating an users identity and preserving useful information.\n","\n","The given dataset consists of Reddit posts (`post_text`) which are annotated with the gender (`op_gender`) of the user and the corresponding subreddit (`subreddit`) category.\n","\n","*  `subreddit_classifier.pickle` pretrained subreddit classifier\n","*  `gender_classifier.pickle` pretrained gender classifier\n","*  `test.csv` your primary test data\n","*  `male.txt` a list of words commonly used by men\n","*  `female.txt` a list of words commonly used by women\n","*  `background.csv` additional Reddit posts that you may optionally use for training an obfuscation model"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"AvClU2_3dpsp","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657223070101,"user_tz":-120,"elapsed":2079,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}},"outputId":"608f5be6-f748-43bb-9fb7-e442724ed4eb"},"outputs":[{"output_type":"stream","name":"stderr","text":["[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":2}],"source":["from sklearn.metrics import accuracy_score\n","from pandas.core.frame import DataFrame\n","from typing import List, Tuple\n","import pandas\n","import pickle\n","import random\n","import nltk\n","nltk.download('punkt')\n","nltk.download('stopwords')"]},{"cell_type":"code","source":["import os"],"metadata":{"id":"lz7BdXfus8FW","executionInfo":{"status":"ok","timestamp":1657223070101,"user_tz":-120,"elapsed":4,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}}},"execution_count":3,"outputs":[]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Q5oiaDj5QNq9","executionInfo":{"status":"ok","timestamp":1657223070101,"user_tz":-120,"elapsed":3,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}}},"outputs":[],"source":["def get_preds(cache_name: str, test: List[str]) -> List[str]:\n","    loaded_model, dictionary, transpose, train_bow = pickle.load(open(cache_name, 'rb'))\n","    X_test = transpose(test, train_bow, dictionary)\n","    preds = loaded_model.predict(X_test)\n","    return preds"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"KoT-ItJ40d1k","executionInfo":{"status":"ok","timestamp":1657223070102,"user_tz":-120,"elapsed":4,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}}},"outputs":[],"source":["def run_classifier(test_file: str, googleDrive=False) -> Tuple[float]:\n","  if googleDrive:\n","    GoogleDrivePathBase='/content/drive/MyDrive/Ethic_in_NLP/HW04/data'\n","  else:\n","    GoogleDrivePathBase=''\n","\n","  test_file=os.path.join(GoogleDrivePathBase,test_file)\n","  test_data = pandas.read_csv(test_file)\n","\n","  cache_name = os.path.join(GoogleDrivePathBase,'gender_classifier.pickle')\n","  test_preds = get_preds(cache_name, list(test_data[\"post_text\"]))\n","  gold_test = list(test_data[\"op_gender\"])\n","  gender_acc = accuracy_score(list(test_preds), gold_test)\n","  print(\"Gender classification accuracy\", gender_acc)\n","\n","  cache_name = os.path.join(GoogleDrivePathBase,'subreddit_classifier.pickle')\n","  test_preds = get_preds(cache_name, list(test_data[\"post_text\"]))\n","  gold_test = list(test_data[\"subreddit\"])\n","  subreddit_acc = accuracy_score(list(test_preds), gold_test)\n","  print(\"Subreddit classification accuracy\", subreddit_acc)\n","  return gender_acc, subreddit_acc"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"bf7nYEb0QPtU","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1657223080634,"user_tz":-120,"elapsed":10536,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}},"outputId":"f329db18-6b08-427d-d9ae-923002882ad7"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gender classification accuracy 0.646\n","Subreddit classification accuracy 0.832\n"]}],"source":["gender_acc, subreddit_acc = run_classifier(\"test.csv\",True)\n","\n","assert gender_acc == 0.646\n","assert subreddit_acc == 0.832"]},{"cell_type":"markdown","metadata":{"id":"9LKaoI6TZJpt"},"source":["**Default accuracy:**\n","*   `Gender    classification accuracy: 0.646`\n","*   `Subreddit classification accuracy: 0.832`"]},{"cell_type":"markdown","metadata":{"id":"-Zl4YplkJgcQ"},"source":["## 2. Obfuscation of the Test Dataset\n","### 2.1 Random Obfuscated Dataset  (4P)\n","First, run a random experiment, by randomly swapping gender-specific words that appear in posts with a word from the respective list of words of the opposite gender.\n","\n","*  Write a function to read the female.txt and male.txt files\n","*  Tokenize the posts („post_text“) using NLTK (0.5p)\n","*  For each post, if written by a man („M“) and containing a token from the male.txt, replace that token with a random one from the female.txt (1p)\n","*  For each post, if written by a woman („W“) and containing a token from the female.txt, replace that token with a random one from the male.txt (1p)\n","*  Save the obfuscated version of the test.csv in a separate csv file (using pandas and makes sure to name them accordingly) (0.5p)\n","*  Run the given classifier again, report the accuracy and provide a brief commentary on the results compared to the baseline (1p)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"P8g4VhvJ7uXD","executionInfo":{"status":"ok","timestamp":1657223080634,"user_tz":-120,"elapsed":3,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}}},"outputs":[],"source":["def read_data(file_name: str) -> List[str]:\n","    \"\"\"\n","    \n","    add your code here\n","\n","    \"\"\"\n","    content=[]\n","    with open(file_name, 'r') as f:\n","      for line in f:\n","        content.append(line.strip())\n","\n","    return content"]},{"cell_type":"code","execution_count":49,"metadata":{"id":"UAcIf9Ck521o","executionInfo":{"status":"ok","timestamp":1657228702603,"user_tz":-120,"elapsed":298,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}}},"outputs":[],"source":["BASE='/content/drive/MyDrive/Ethic_in_NLP/HW04/data'\n","\n","MALE_PATH=os.path.join(BASE, 'male.txt')\n","FEMELE_PATH=os.path.join(BASE, 'female.txt')\n","\n","male_words = read_data(MALE_PATH)\n","female_words = read_data(FEMELE_PATH)\n","\n","assert len(male_words) == 3000\n","assert len(male_words) == 3000"]},{"cell_type":"code","source":["from nltk.corpus import stopwords\n","import random\n","\n","SEED=123\n","random.seed(SEED)"],"metadata":{"id":"q8s4nQJWQpJ2","executionInfo":{"status":"ok","timestamp":1657223080635,"user_tz":-120,"elapsed":4,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}}},"execution_count":9,"outputs":[]},{"cell_type":"code","execution_count":10,"metadata":{"id":"wB1a1TuP8A01","executionInfo":{"status":"ok","timestamp":1657223080635,"user_tz":-120,"elapsed":3,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}}},"outputs":[],"source":["def customized_tokenize(sentence:str):\n","  stopWords = set(stopwords.words('english'))\n","  if  not sentence:\n","    return ' '\n","  else:\n","    return  [word.lower() for word in nltk.word_tokenize(sentence) if word not in stopWords and word.isalpha()]\n","\n","def replace(gender_words: List[str], opposite_gender_words: List[str],raw_sequence:List[str]):\n","  new_sequence=[]\n","  for word in raw_sequence:\n","    if word in gender_words:\n","      while True:\n","        # avoid being replaced with punctuations\n","        idx=random.randint(0,len(opposite_gender_words)-1)\n","        newword=opposite_gender_words[idx]\n","        if newword.isalpha():\n","          break\n","      new_sequence.append(newword)\n","    else:\n","      new_sequence.append(word)\n","  \n","  return ' '.join(new_sequence)\n","\n","\n","def randomly_replace(male_words: List[str], female_words: List[str], item:pandas.Series):\n","  if item.op_gender=='M':\n","    item['post_text']=replace(male_words, female_words, item['post_text'])\n","\n","  elif item.op_gender=='W':\n","    item['post_text']=replace(female_words, male_words, item['post_text'])\n","  else: \n","    raise ValueError('The gender of the user is not explicitly mentioned !!')\n","\n","  return item\n","\n","def obfuscate_gender_randomly(male_words: List[str], female_words: List[str], dataset_file_name: str) -> DataFrame:\n","  \"\"\"\n","  \n","  add your code here\n","  \n","  \"\"\"\n","  df=pandas.read_csv(dataset_file_name, sep=',', encoding='utf-8',header=0)\n","  df['post_text']=df['post_text'].apply(lambda x: customized_tokenize(x))\n","\n","  df=df.apply(lambda x : randomly_replace(male_words, female_words, x),axis=1)\n","  \n","  # replace blank value with NaN\n","  df['post_text'] = df['post_text'].apply(lambda x: x.strip()).replace('', pandas.NA)\n","\n","  # replace nan with new value\n","  df['post_text']=df['post_text'].replace(to_replace = pandas.NA, value =-99999)\n","\n","  if df.post_text.isna().any():\n","    raise ValueError('There are some NaN in the column \"post_text\" ')\n","  \n","  print(type(df.loc[df['op_id']=='ninepointsix'].post_text))\n","\n","  \n","\n","  return df\n"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"TzXQAIxJg9Hn","executionInfo":{"status":"ok","timestamp":1657223080635,"user_tz":-120,"elapsed":3,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}}},"outputs":[],"source":["file_name = \"randomReplacement_tokenized_testset.csv\""]},{"cell_type":"markdown","source":[""],"metadata":{"id":"eS61OED9eoIe"}},{"cell_type":"code","execution_count":12,"metadata":{"id":"QJLnNKpwwtqo","executionInfo":{"status":"ok","timestamp":1657223081729,"user_tz":-120,"elapsed":1097,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"8e89d164-610b-418b-9477-cab7f29d1c9d"},"outputs":[{"output_type":"stream","name":"stdout","text":["<class 'pandas.core.series.Series'>\n"]}],"source":["TEST_PATH=os.path.join(BASE, 'test.csv')\n","SAVE_PATH=os.path.join(BASE, file_name)\n","random_replaced_test = obfuscate_gender_randomly(male_words=male_words, female_words=female_words, dataset_file_name=TEST_PATH)\n","random_replaced_test.to_csv(SAVE_PATH)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"xNiDfkgPm9DY","executionInfo":{"status":"ok","timestamp":1657223081729,"user_tz":-120,"elapsed":2,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}}},"outputs":[],"source":["random_replaced_test = pandas.read_csv(SAVE_PATH)\n","assert len(random_replaced_test) == 500\n","assert random_replaced_test[\"subreddit\"][0] == \"funny\"\n","assert random_replaced_test[\"subreddit\"][-1:].item() == \"relationships\""]},{"cell_type":"code","execution_count":14,"metadata":{"id":"osyy2rw9J8X1","executionInfo":{"status":"ok","timestamp":1657223085588,"user_tz":-120,"elapsed":3861,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"25ffb440-4aba-4660-d7b8-65e6c8824f81"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gender classification accuracy 0.48\n","Subreddit classification accuracy 0.788\n"]}],"source":["gender_acc, subreddit_acc = run_classifier(file_name, True)\n","\n","assert gender_acc <= 0.5\n","assert subreddit_acc >= 0.7"]},{"cell_type":"markdown","metadata":{"id":"S9AsCLRtYXc8"},"source":["**Report accuracy:**\n","*   `Gender    classification accuracy: ` 0.48\n","*   `Subreddit classification accuracy: ` 0.788\n","*   `Your commentary: ` In comparison with the baseline, the random word replcement method can obfuscate demographic features, the gender classification accuracy decreased from 64.6% to 48%, however this reduction is achieved at the expense of the change of the original paraphrase, as the subreddit classification accuracy also decrease from 0.83.2 to 0.788 "]},{"cell_type":"markdown","metadata":{"id":"4OHsp4B7Jqnv"},"source":["### 2.2 Similarity Obfuscated Dataset (4P)\n","In a second approach, refine the swap method. Instead of randomly selecting a word, use a similarity metric.\n","\n","\n","*  Instead of the first method replace the tokens by semantically similar tokens from the other genders token list. For that you may choose any metric for identifying semantically similar words, but you have to justify your choice. (Recommend: using cosine distance between pre-trained word embeddings) (2p)\n","*  Save the obfuscated version of the test.csv in a separate CSV file (using pandas and makes sure to name them accordingly) (0.5p)\n","*  Run the given classifier again, report the accuracy and provide a brief commentary on the results (compared to the baseline and your other results) (1p)\n","*  The classifiers accuracy for predicting the gender should be below random guessing (50%) and for the subreddit prediction it should be above 80% (0.5p)"]},{"cell_type":"code","source":["import gensim.downloader\n","import gensim\n","# Show all available models in gensim-data\n","print(list(gensim.downloader.info()['models'].keys()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"BmOh_YFXzSu0","executionInfo":{"status":"ok","timestamp":1657223085588,"user_tz":-120,"elapsed":9,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}},"outputId":"51722dc1-13c0-4fed-c159-3087c46239f2"},"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"]}]},{"cell_type":"code","source":["glove_vectors = gensim.downloader.load('glove-twitter-50')\n","# from gensim.models import KeyedVectors \n","# # glove_vectors=gensim.models.Word2Vec.load('/content/drive/MyDrive/Ethic_in_NLP/HW04/model/word2vec-google-news-300/word2vec-google-news-300.model')\n","# glove_vectors=KeyedVectors.load('/content/drive/MyDrive/Ethic_in_NLP/HW04/model/word2vec-google-news-300/word2vec-google-news-300.model')\n"],"metadata":{"id":"lcQE2jek0z2z","executionInfo":{"status":"ok","timestamp":1657223128519,"user_tz":-120,"elapsed":42939,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}}},"execution_count":16,"outputs":[]},{"cell_type":"code","source":["(glove_vectors.most_similar('effort',topn=30))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_ub3BMT41G1X","executionInfo":{"status":"ok","timestamp":1657223128929,"user_tz":-120,"elapsed":418,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}},"outputId":"32565adf-e3a1-4d2f-8f48-42fa993801f5"},"execution_count":17,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('advantage', 0.7885393500328064),\n"," ('efforts', 0.7689546346664429),\n"," ('positive', 0.7641745209693909),\n"," ('difficult', 0.7637898325920105),\n"," ('success', 0.7618691921234131),\n"," ('confidence', 0.7613573670387268),\n"," ('difference', 0.7592354416847229),\n"," ('expect', 0.7585071325302124),\n"," ('important', 0.7489951848983765),\n"," ('enough', 0.748828113079071),\n"," ('motivation', 0.7486553192138672),\n"," ('progress', 0.7485166788101196),\n"," ('patience', 0.7442478537559509),\n"," ('decision', 0.74277663230896),\n"," ('willing', 0.7402777671813965),\n"," ('example', 0.7390508651733398),\n"," ('commitment', 0.7374849319458008),\n"," ('push', 0.7373062372207642),\n"," ('appreciate', 0.7368417978286743),\n"," ('however', 0.7357004284858704),\n"," ('timing', 0.7339947819709778),\n"," ('tough', 0.73281329870224),\n"," ('although', 0.7287693619728088),\n"," ('sense', 0.7284727692604065),\n"," ('matter', 0.7276229858398438),\n"," ('lack', 0.72706139087677),\n"," ('opportunity', 0.726543664932251),\n"," ('focus', 0.7243965268135071),\n"," ('ability', 0.7236769199371338),\n"," ('consistent', 0.7235769629478455)]"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","execution_count":18,"metadata":{"id":"mDblihF2fkOh","executionInfo":{"status":"ok","timestamp":1657223128929,"user_tz":-120,"elapsed":4,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}}},"outputs":[],"source":["def similarity_replace(gender_words: List[str], opposite_gender_words: List[str],raw_sequence:List[str]):\n","  new_sequence=[]\n","  model=glove_vectors\n","  if not model:\n","    raise ValueError('There are no pre-trained model!')\n","\n","  for word in raw_sequence:\n","    if word in gender_words:\n","      candidates=[]\n","\n","      # If the word does not exist in the vocabulary of the model\n","      # then, use the word itself as the candidate\n","      try:\n","        candidates=model.most_similar(word,topn=20)\n","      except:\n","        candidates.append(tuple(word))\n","\n","      replaced_tag=False\n","      for candidate in candidates:\n","        if candidate[0] in opposite_gender_words and candidate[0].isalpha(): # avoid being replaced with punctuations\n","          new_sequence.append(candidate[0])\n","          replaced_tag=True\n","          break    \n","      \n","      # if there are no such word from opposite_gender_words which is \n","      # similar to the word in raw_sequence(original word),\n","      # the word will be replaced randomly. \n","      if not replaced_tag:\n","        while True:\n","          # avoid being replaced with punctuations\n","          idx=random.randint(0,len(opposite_gender_words)-1)\n","          newword=opposite_gender_words[idx]\n","          if newword.isalpha():\n","            break\n","        new_sequence.append(newword)\n","        # new_sequence.append(word)\n","\n","\n","    else:\n","      new_sequence.append(word)\n","  \n","  return ' '.join(new_sequence)\n","\n","\n","def similarity_replacement(male_words: List[str], female_words: List[str], item:pandas.Series):\n","\n","  if item.op_gender=='M':\n","    item['post_text']=similarity_replace(male_words, female_words, item['post_text'])\n","\n","  elif item.op_gender=='W':\n","    item['post_text']=similarity_replace(female_words, male_words, item['post_text'])\n","  else: \n","    raise ValueError('The gender of the user is not explicitly mentioned !!')\n","\n","  return item\n","\n","\n","\n","def obfuscate_gender_by_similarity(male_words: List[str], female_words: List[str], dataset_file_name: str) -> DataFrame:\n","  \"\"\"\n","  \n","  add your code here\n","  \n","  \"\"\"\n","  df=pandas.read_csv(dataset_file_name, sep=',', encoding='utf-8',header=0)\n","  df['post_text']=df['post_text'].apply(lambda x: customized_tokenize(x))\n","\n","  df=df.apply(lambda x : similarity_replacement(male_words, female_words, x),axis=1)\n","  \n","  # replace blank value with NaN\n","  df['post_text'] = df['post_text'].apply(lambda x: x.strip()).replace('', pandas.NA)\n","\n","  # replace nan with new value\n","  df['post_text']=df['post_text'].replace(to_replace = pandas.NA, value =-99999)\n","\n","  if df.post_text.isna().any():\n","    raise ValueError('There are some NaN in the column \"post_text\" ')\n","\n","\n","\n","  return df"]},{"cell_type":"code","execution_count":19,"metadata":{"id":"kYAS5eXqcEoe","executionInfo":{"status":"ok","timestamp":1657223128929,"user_tz":-120,"elapsed":3,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}},"colab":{"base_uri":"https://localhost:8080/","height":36},"outputId":"ac4ca9b1-62cc-414a-a430-d978fb518fc2"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'\\n you may use gensim models for example word2vec-google-news-300\\n'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":19}],"source":["\"\"\"\n"," you may use gensim models for example word2vec-google-news-300\n","\"\"\""]},{"cell_type":"code","execution_count":20,"metadata":{"id":"Oo1vZ0aZhHb6","executionInfo":{"status":"ok","timestamp":1657223128930,"user_tz":-120,"elapsed":4,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}}},"outputs":[],"source":["file_name = \"similarity_tokenized_testset.csv\""]},{"cell_type":"markdown","source":["* The below code block would take almost **14 minutes** on the colab w.r.t. no \n","restriction on the len(word)\n","* The below code block would take almost **6 minutes** on the colab w.r.t. len(word) has to be larger than 3"],"metadata":{"id":"O55te-JVCnyy"}},{"cell_type":"code","execution_count":21,"metadata":{"id":"1DPcGRD06UFN","executionInfo":{"status":"ok","timestamp":1657223254209,"user_tz":-120,"elapsed":125282,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}}},"outputs":[],"source":["SAVE_Similarity_PATH=os.path.join(BASE,file_name)\n","similarity_replaced_test = obfuscate_gender_by_similarity(male_words=male_words, female_words=female_words, dataset_file_name=TEST_PATH)\n","similarity_replaced_test.to_csv(SAVE_Similarity_PATH)"]},{"cell_type":"code","source":["similarity_replaced_test.loc[0].post_text"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":36},"id":"oga3YuIvyceD","executionInfo":{"status":"ok","timestamp":1657223254211,"user_tz":-120,"elapsed":12,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}},"outputId":"37adc824-6b88-4a7a-d577-d4962465c3cf"},"execution_count":22,"outputs":[{"output_type":"execute_result","data":{"text/plain":["'patience nevermind kidding cuz pretty sure boys help russian studies'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":22}]},{"cell_type":"code","execution_count":23,"metadata":{"id":"vl6yL2mTmDRX","executionInfo":{"status":"ok","timestamp":1657223254211,"user_tz":-120,"elapsed":11,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}}},"outputs":[],"source":["similarity_replaced_test = pandas.read_csv(SAVE_Similarity_PATH)\n","assert len(similarity_replaced_test) == 500\n","assert similarity_replaced_test[\"subreddit\"][0] == \"funny\"\n","assert similarity_replaced_test[\"subreddit\"][-1:].item() == \"relationships\""]},{"cell_type":"code","execution_count":24,"metadata":{"id":"LAgAPPSrLWsK","executionInfo":{"status":"ok","timestamp":1657223257880,"user_tz":-120,"elapsed":3679,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"47134df8-e826-4c8d-a196-7f22133f1c77"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gender classification accuracy 0.478\n","Subreddit classification accuracy 0.808\n"]}],"source":["gender_acc, subreddit_acc = run_classifier(file_name,True)\n","\n","assert gender_acc <= 0.5\n","assert subreddit_acc >= 0.8\n"]},{"cell_type":"markdown","metadata":{"id":"gapuF5CTZCK-"},"source":["**Report accuracy:**\n","*   `Gender    classification accuracy: ` 0.478\n","*   `Subreddit classification accuracy: ` 0.808\n","*   `Your commentary: ` The obfuscation capability of the lexical similarity replacement method is closed to the random method above, as their gender classification accuracies are closed. Besides, the similarity method outperforms the random method in the aspect of persevering the paraphrase of the original sentence. "]},{"cell_type":"markdown","source":["By using goolge-news-300 model, the gender_acc, subreddit_acc reaches 0.562, 0.808 respectively\n","\n","By using glove-twitter-50 model, the gender_acc, subreddit_acc reaches 0.476, 0.806 respectively"],"metadata":{"id":"9281tSd7KXwq"}},{"cell_type":"markdown","metadata":{"id":"mkdkD89GoxDo"},"source":["### 2.3 Your Own Obfuscated Dataset (4P)\n","With this last approach, you can experiment by yourself how to obfuscate the posts.\n","\n","*  Some examples: What if you randomly decide whether or not to replace words instead of replacing every lexicon word? What if you only replace words that have semantically similar enough counterparts? What if you use different word embeddings? (2p)\n","*  Save the obfuscated version of the test.csv in a separate csv file (using pandas and makes sure to name them accordingly) (0.5p)\n","*  Describe your modifications and report the accuracy and provide a brief commentary on the results compared to the baseline and your other results (1.5p)"]},{"cell_type":"code","source":["print(list(gensim.downloader.info()['models'].keys()))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"UMXK9O9Hc1OU","executionInfo":{"status":"ok","timestamp":1657223257881,"user_tz":-120,"elapsed":10,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}},"outputId":"80c1707c-4b78-4766-9d36-70195bf5f20a"},"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["['fasttext-wiki-news-subwords-300', 'conceptnet-numberbatch-17-06-300', 'word2vec-ruscorpora-300', 'word2vec-google-news-300', 'glove-wiki-gigaword-50', 'glove-wiki-gigaword-100', 'glove-wiki-gigaword-200', 'glove-wiki-gigaword-300', 'glove-twitter-25', 'glove-twitter-50', 'glove-twitter-100', 'glove-twitter-200', '__testing_word2vec-matrix-synopsis']\n"]}]},{"cell_type":"code","source":["glove_vectors_twitter_100 = gensim.downloader.load('glove-twitter-100')"],"metadata":{"id":"2BsKjWwiZiKV","executionInfo":{"status":"ok","timestamp":1657223335272,"user_tz":-120,"elapsed":77399,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["glove_vectors_twitter_100['word']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IDVXTt__kq86","executionInfo":{"status":"ok","timestamp":1657230605007,"user_tz":-120,"elapsed":286,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}},"outputId":"f01fb50a-0b92-44ad-83a9-0d3d45a3f4f3"},"execution_count":71,"outputs":[{"output_type":"execute_result","data":{"text/plain":["array([ 0.57479 ,  0.27959 , -0.17003 ,  1.0926  , -0.5678  ,  0.13946 ,\n","       -0.22845 ,  0.27979 ,  0.1436  ,  0.25408 ,  0.14175 ,  0.47737 ,\n","       -4.1063  , -0.45932 , -0.78775 , -0.061295,  0.28098 ,  0.55691 ,\n","        0.040097, -0.33675 ,  0.10952 ,  0.32482 , -0.60996 ,  0.77837 ,\n","        1.0855  ,  0.092512, -0.34347 , -0.52561 , -0.32974 , -0.45062 ,\n","       -0.33763 ,  0.26943 , -0.7608  , -0.013459, -0.097348, -0.40263 ,\n","        0.22523 ,  0.40602 ,  0.34765 , -1.2264  , -0.81516 , -0.57451 ,\n","        0.084248,  0.36518 ,  0.24649 , -0.26708 ,  0.074   ,  0.73033 ,\n","       -0.34619 ,  0.29964 ,  0.49903 ,  0.46251 , -0.68305 , -0.92597 ,\n","        0.075895, -0.51661 , -0.67615 , -0.017943, -1.1911  , -0.12817 ,\n","        0.27478 , -0.77928 , -0.35465 ,  0.39712 ,  0.22347 ,  0.38169 ,\n","       -0.067566, -0.24608 ,  0.34249 , -0.26701 , -0.78815 , -0.79426 ,\n","       -0.57019 ,  0.14404 ,  0.23621 , -0.067121,  0.31948 ,  0.06233 ,\n","       -0.3619  , -0.012909,  0.91253 ,  0.21408 ,  0.12472 , -0.64596 ,\n","        0.12799 ,  0.11704 , -0.66266 ,  0.31085 , -0.3327  , -0.2197  ,\n","        0.4885  ,  0.42261 , -0.19855 , -0.081162,  0.015766, -0.12767 ,\n","       -0.062801,  0.67944 ,  0.65676 , -0.19021 ], dtype=float32)"]},"metadata":{},"execution_count":71}]},{"cell_type":"code","source":["glove_vectors_word2vec_google_news_300 = gensim.downloader.load('word2vec-google-news-300')"],"metadata":{"id":"2NeVKWAZc-BB","executionInfo":{"status":"ok","timestamp":1657223454100,"user_tz":-120,"elapsed":118836,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}}},"execution_count":27,"outputs":[]},{"cell_type":"code","source":["def customized_replace(gender_words: List[str], opposite_gender_words: List[str],raw_sequence:List[str]):\n","  new_sequence=[]\n","  model=glove_vectors_twitter_100\n","  if not model:\n","    raise ValueError('There are no pre-trained model!')\n","\n","  for word in raw_sequence:\n","    candidates=[]\n","    trigger==random.choice([0,1])\n","    if trigger:\n","      try:\n","        new_sequence.append(model.most_similar(word, topn=1)[0][0])\n","\n","      except:\n","        new_sequence.append(word)\n","  \n","  return ' '.join(new_sequence)\n","\n","\n","def customiezed_replacement(male_words: List[str], female_words: List[str], item:pandas.Series):\n","  \n","  \n","  if item.op_gender=='M':\n","    item['post_text']=customized_replace(male_words, female_words, item['post_text'])\n","\n","  elif item.op_gender=='W':\n","    item['post_text']=customized_replace(female_words, male_words, item['post_text'])\n","  else: \n","    raise ValueError('The gender of the user is not explicitly mentioned !!')\n","\n","  return item\n","\n","\n","\n","\n","def obfuscate_gender(male_words: List[str], female_words: List[str], dataset_file_name: str) -> DataFrame:\n","  \"\"\"\n","\n","    add your own implemntation, you may add more functions and arguments\n","    \n","  \"\"\"\n","\n","  df=pandas.read_csv(dataset_file_name, sep=',', encoding='utf-8',header=0)\n","  df['post_text']=df['post_text'].apply(lambda x: customized_tokenize(x))\n","\n","  df=df.apply(lambda x : similarity_replacement(male_words, female_words, x),axis=1)\n","  \n","  # replace blank value with NaN\n","  df['post_text'] = df['post_text'].apply(lambda x: x.strip()).replace('', pandas.NA)\n","\n","  # replace nan with new value\n","  df['post_text']=df['post_text'].replace(to_replace = pandas.NA, value =-99999)\n","\n","  if df.post_text.isna().any():\n","    raise ValueError('There are some NaN in the column \"post_text\" ')\n","\n","\n","\n","  return df"],"metadata":{"id":"5mmmxiTp0SfP","executionInfo":{"status":"ok","timestamp":1657229619071,"user_tz":-120,"elapsed":282,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","execution_count":63,"metadata":{"id":"HD9heixWhpHE","executionInfo":{"status":"ok","timestamp":1657229624361,"user_tz":-120,"elapsed":289,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}}},"outputs":[],"source":["file_name = \"costumized_tokenized_testset.csv\""]},{"cell_type":"code","source":["SAVE_Custumized_PATH=os.path.join(BASE,file_name)\n","your_test = obfuscate_gender(male_words=male_words, female_words=female_words, dataset_file_name=TEST_PATH)\n","your_test.to_csv(SAVE_Custumized_PATH)"],"metadata":{"id":"ae7n8UGNIXz_","executionInfo":{"status":"ok","timestamp":1657229811029,"user_tz":-120,"elapsed":185297,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}}},"execution_count":64,"outputs":[]},{"cell_type":"code","execution_count":66,"metadata":{"id":"zXJVOsePuEs7","executionInfo":{"status":"ok","timestamp":1657229811031,"user_tz":-120,"elapsed":5,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}}},"outputs":[],"source":["your_test = pandas.read_csv(SAVE_Custumized_PATH)\n","assert len(your_test) == 500\n","assert your_test[\"subreddit\"][0] == \"funny\"\n","assert your_test[\"subreddit\"][-1:].item() == \"relationships\""]},{"cell_type":"code","execution_count":67,"metadata":{"id":"MO1tJMhzoqsL","executionInfo":{"status":"ok","timestamp":1657229815252,"user_tz":-120,"elapsed":4225,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"78e0f5be-c4d7-4732-ecc8-523993b4cae5"},"outputs":[{"output_type":"stream","name":"stdout","text":["Gender classification accuracy 0.466\n","Subreddit classification accuracy 0.812\n"]}],"source":["gender_acc, subreddit_acc = run_classifier(file_name,True)\n","\n","assert gender_acc <= 0.5\n","assert subreddit_acc >= 0.6"]},{"cell_type":"markdown","metadata":{"id":"US3Gcok5qdYo"},"source":["**Report accuracy:**\n","*   `Gender    classification accuracy: ` 0.466\n","*   `Subreddit classification accuracy: ` 0.812\n","*   `Your commentary: ` \n","\n","I tried to use different set of parameters, such as word embedding, only replace words that have semantically similar enough counterparts, i.e. only the most similar word is replaced. The table below shows the performances of different methods. The best one is using 'glove-twitter-50', 7 similar words and randomly decide whether replace word or not. \n","\n","\n","\n","| Parameters                                   | Gender classification accuracy | Subreddit classification accuracy |\n","|----------------------------------------------|--------------------------------|-----------------------------------|\n","| (glove-twitter-50, 10 similiar word, random) | 0.474                          | 0.808                             |\n","| (glove-twitter-50, 7 similiar word, random)  | 0.466                          | 0.812                              |\n","| (glove-twitter-50, 5 similiar word, random)  | 0.472                          | 0.804                             |\n","| (glove-twitter-50, 7 similiar word, remain)  | 0.47                           | 0.808                             |\n","| (glove-twitter-100, 7 similiar word, random) | 0.474                          | 0.806                             |\n"]},{"cell_type":"markdown","metadata":{"id":"48j9qFLJFygB"},"source":["### 3 Advanced Obfuscated Model (5P)\n","Develop your own obfuscation model using the provided background.csv for training. Your ultimate goal should be to obfuscate text so that the classifier is unable to determine the gender of an user (no better than random guessing) without compromising the accuracy of the subreddit classification task. To train a model that is good at predicting subreddit classification, but bad at predicting gender. The key idea in this approach is to design a model that does not encode information about protected attributes (in this case, gender). In your report, include a description of your model and results.\n","\n","*  Develop your own classifier (3p)\n","*  Use only posts from the subreddits „CasualConversation“ and „funny“ (min. 1000 posts for each gender per subreddit) (0.5p)\n","*  Use sklearn models (MLPClassifier, LogisticRegression, etc.)\n","*  Use 90% for training and 10% for testing (0.5p)\n","*  In your report, include a description of your model and report the accuracy on the unmodified train data (your baseline here) as well as the modified train data and provide a brief commentary on the results (1p)"]},{"cell_type":"code","source":["\"\"\"\n","\n","add your code here\n","\n","The below code are still bugy\n","\n","\"\"\"\n","\n","# load data and preprocessing\n","background_data=pandas.read_csv(os.path.join(BASE, 'background.csv'))\n","background_data['post_text']=background_data['post_text'].apply(lambda x: customized_tokenize(x))\n","\n","\n","\n","def sentenc_embedding(item,model):\n","  item['op_gender'] = 1 if item['op_gender']=='M' else 0\n","\n","  sent= item['post_text']\n","  sent_vec =[]\n","  numw = 0\n","  for w in sent:\n","      try:\n","          if numw == 0:\n","              sent_vec = model[w]\n","          else:\n","              sent_vec = np.add(sent_vec, model[w])\n","          numw+=1\n","      except:\n","          pass\n","\n","  item['post_text']=np.asarray(sent_vec) / numw\n","    \n","  return item\n","\n","\n","\n"],"metadata":{"id":"KrkWTZJc3uNn","executionInfo":{"status":"ok","timestamp":1657230680443,"user_tz":-120,"elapsed":29194,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}}},"execution_count":72,"outputs":[]},{"cell_type":"code","source":["background_data=background_data.apply(lambda x: sentenc_embedding(x, glove_vectors_twitter_100))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":323},"id":"aeLhVzveiHTu","executionInfo":{"status":"error","timestamp":1657230725168,"user_tz":-120,"elapsed":300,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}},"outputId":"9effaba5-4307-4be1-b280-9237b717f93d"},"execution_count":75,"outputs":[{"output_type":"error","ename":"KeyError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)","\u001b[0;32m<ipython-input-75-ca7e0f93c27a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbackground_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackground_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msentenc_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_vectors_twitter_100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, axis, raw, result_type, args, **kwargs)\u001b[0m\n\u001b[1;32m   8738\u001b[0m             \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8739\u001b[0m         )\n\u001b[0;32m-> 8740\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   8741\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   8742\u001b[0m     def applymap(\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    686\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_raw\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    687\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 688\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    689\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    690\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0magg\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    811\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mapply_standard\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 812\u001b[0;31m         \u001b[0mresults\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mres_index\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_series_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    813\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    814\u001b[0m         \u001b[0;31m# wrap results\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/apply.py\u001b[0m in \u001b[0;36mapply_series_generator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    826\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mseries_gen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m                 \u001b[0;31m# ignore SettingWithCopy here in case the user mutates\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m                 \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mABCSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m                     \u001b[0;31m# If we have a view on v, we need to make a copy because\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-75-ca7e0f93c27a>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbackground_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbackground_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0msentenc_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mglove_vectors_twitter_100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-72-ca0cc4084227>\u001b[0m in \u001b[0;36msentenc_embedding\u001b[0;34m(item, model)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0msentenc_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m   \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'op_gender'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'op_gender'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'M'\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m   \u001b[0msent\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mitem\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'post_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    940\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    941\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mkey_is_scalar\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 942\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    944\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mis_hashable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/series.py\u001b[0m in \u001b[0;36m_get_value\u001b[0;34m(self, label, takeable)\u001b[0m\n\u001b[1;32m   1049\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1050\u001b[0m         \u001b[0;31m# Similar to Index.get_value, but we do not fall back to positional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1051\u001b[0;31m         \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_values_for_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/core/indexes/range.py\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m    386\u001b[0m                 \u001b[0;32mexcept\u001b[0m \u001b[0mValueError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m                     \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 388\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    389\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyError\u001b[0m: 'op_gender'"]}]},{"cell_type":"code","source":["background_data"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":423},"id":"z91cPk8spcY9","executionInfo":{"status":"ok","timestamp":1657230708613,"user_tz":-120,"elapsed":288,"user":{"displayName":"Nun YN","userId":"02929111290423234527"}},"outputId":"d0dc73d2-db90-48db-c7dd-527b27e5a242"},"execution_count":74,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       Unnamed: 0  Unnamed: 0.1                op_id op_gender  post_id  \\\n","0          596598        596598              Delts28         M   596598   \n","1          596599        596599      robertmeowneyjr         M   596599   \n","2          596600        596600              Delts28         M   596600   \n","3          596601        596601              Delts28         M   596601   \n","4          596602        596602              Delts28         M   596602   \n","...           ...           ...                  ...       ...      ...   \n","54744      749530        761059               Morc35         M   761059   \n","54745      749531        761060                O-shi         M   761060   \n","54746      749532        761061          Shenko-wolf         M   761061   \n","54747      749533        761062  i_forget_my_userids         M   761062   \n","54748      749534        761063         Brandon48236         M   761063   \n","\n","                                               post_text           subreddit  \\\n","0                    [wow, still, going, well, im, game]  CasualConversation   \n","1      [you, guys, nice, cars, heres, mine, link, jus...  CasualConversation   \n","2      [two, samsungs, an, netbook, seems, indestruct...  CasualConversation   \n","3      [i, used, marine, engineer, wasnt, life, now, ...  CasualConversation   \n","4      [well, ive, countries, people, dont, know, exi...  CasualConversation   \n","...                                                  ...                 ...   \n","54744                                       [yellow, no]               funny   \n","54745                    [sign, maker, seems, like, fun]               funny   \n","54746                         [those, trophy, zucchinis]               funny   \n","54747  [lol, youre, wrong, good, resource, community,...               funny   \n","54748       [id, pissed, i, strangers, taking, pictures]               funny   \n","\n","       op_gender_visible  \n","0                  False  \n","1                  False  \n","2                  False  \n","3                  False  \n","4                  False  \n","...                  ...  \n","54744              False  \n","54745              False  \n","54746              False  \n","54747              False  \n","54748              False  \n","\n","[54749 rows x 8 columns]"],"text/html":["\n","  <div id=\"df-15016b3c-148f-4807-9199-391bbac2f72d\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>Unnamed: 0</th>\n","      <th>Unnamed: 0.1</th>\n","      <th>op_id</th>\n","      <th>op_gender</th>\n","      <th>post_id</th>\n","      <th>post_text</th>\n","      <th>subreddit</th>\n","      <th>op_gender_visible</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>596598</td>\n","      <td>596598</td>\n","      <td>Delts28</td>\n","      <td>M</td>\n","      <td>596598</td>\n","      <td>[wow, still, going, well, im, game]</td>\n","      <td>CasualConversation</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>596599</td>\n","      <td>596599</td>\n","      <td>robertmeowneyjr</td>\n","      <td>M</td>\n","      <td>596599</td>\n","      <td>[you, guys, nice, cars, heres, mine, link, jus...</td>\n","      <td>CasualConversation</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>596600</td>\n","      <td>596600</td>\n","      <td>Delts28</td>\n","      <td>M</td>\n","      <td>596600</td>\n","      <td>[two, samsungs, an, netbook, seems, indestruct...</td>\n","      <td>CasualConversation</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>596601</td>\n","      <td>596601</td>\n","      <td>Delts28</td>\n","      <td>M</td>\n","      <td>596601</td>\n","      <td>[i, used, marine, engineer, wasnt, life, now, ...</td>\n","      <td>CasualConversation</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>596602</td>\n","      <td>596602</td>\n","      <td>Delts28</td>\n","      <td>M</td>\n","      <td>596602</td>\n","      <td>[well, ive, countries, people, dont, know, exi...</td>\n","      <td>CasualConversation</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>54744</th>\n","      <td>749530</td>\n","      <td>761059</td>\n","      <td>Morc35</td>\n","      <td>M</td>\n","      <td>761059</td>\n","      <td>[yellow, no]</td>\n","      <td>funny</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>54745</th>\n","      <td>749531</td>\n","      <td>761060</td>\n","      <td>O-shi</td>\n","      <td>M</td>\n","      <td>761060</td>\n","      <td>[sign, maker, seems, like, fun]</td>\n","      <td>funny</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>54746</th>\n","      <td>749532</td>\n","      <td>761061</td>\n","      <td>Shenko-wolf</td>\n","      <td>M</td>\n","      <td>761061</td>\n","      <td>[those, trophy, zucchinis]</td>\n","      <td>funny</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>54747</th>\n","      <td>749533</td>\n","      <td>761062</td>\n","      <td>i_forget_my_userids</td>\n","      <td>M</td>\n","      <td>761062</td>\n","      <td>[lol, youre, wrong, good, resource, community,...</td>\n","      <td>funny</td>\n","      <td>False</td>\n","    </tr>\n","    <tr>\n","      <th>54748</th>\n","      <td>749534</td>\n","      <td>761063</td>\n","      <td>Brandon48236</td>\n","      <td>M</td>\n","      <td>761063</td>\n","      <td>[id, pissed, i, strangers, taking, pictures]</td>\n","      <td>funny</td>\n","      <td>False</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>54749 rows × 8 columns</p>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-15016b3c-148f-4807-9199-391bbac2f72d')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-15016b3c-148f-4807-9199-391bbac2f72d button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-15016b3c-148f-4807-9199-391bbac2f72d');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":74}]},{"cell_type":"markdown","metadata":{"id":"KoePMqHbZE9p"},"source":["**Report accuracy:**\n","* Baseline:\n","  * `Gender    classification accuracy: `\n","  * `Subreddit classification accuracy: `\n","* Your Model: \n","  * `Gender    classification accuracy: `\n","  * `Subreddit classification accuracy: ` \n","*   `Your commentary: ` ..."]},{"cell_type":"markdown","metadata":{"id":"JZERIDpgwnj_"},"source":["### 4 Ethical Implications (3P)\n","Discuss the ethical implications of obfuscation and privacy based on the concepts covered in the lecture. Provide answers to the following points:\n","\n","1.   What are demographic features (name at least three) and explain shortly some of the privacy violation risks? (1p)\n","2.   Explain the cultural and social implications and their effects? In this context discuss the information privacy paradox. You may refer to a recent example like the COVID-19 pandemic.  (1.5p)\n","3.   Name a at least three privacy preserving countermeasures  (0.5p)"]},{"cell_type":"markdown","metadata":{"id":"6Q_Qbw0xw3W0"},"source":["Your Answer: ...\n","\n","**Question 1**\n","\n","Demographic features are statisitical factors that influence population growth or decline, including age structure, fecundity, sex ratio etc. The privacy violation might lead to the increase of bureaucracy and the abuse of power\n","\n","**Question 2**\n","\n","Citizens' personal privacy and security are in a paradoxical relationship. On the one hand, extensive public monitoring and censorship of content on social media is necessary to deter crime and prevent crime. However, centralizing these responsibilities and efforts in one agency can lead to increased bureaucracy and abuse of power. The trade-off between privacy and security are different among different cultures and countries. \n","\n","In the COVID19 pandemic, the different policies regarding different countries are an illustrative instance of the above theory. In some countries, the ordinary people believe the government has no right to track them even if their data are promised to be only used for monitoring plague, which results in the unavoidable and significant increase of people injected by the COIVD virus. In some countries, the amount of injected people are under control by large-scale tracking citizen and collective quarantine. However in those countries, there is sometimes many of news that some officials used the tracking technologies to threaten citizens who were going to reveal corruption. \n","\n","> Phone tapping and surveillance of suspects is the best way to find terrorist before they perform an attack ---- Sir Humphrey Appleby, *Yes, Minister*\n","\n","\n","**Question 3**\n","\n","1. People use PGP, a protocol/framework aiming to protect data privacy by\n","crptographic signature, to send and receive individual Email. \n","2. The ePrivacy Diective, a privacy legislatino that requires sites to get conset from visitors befor placing cookies ontheir devices, prevents the privider  of a website from abusing cookies to track visitors. \n","3. Virtual private network(VPN) and Onion Router(Tor) enable anonymous communication. \n","\n","\n"]}],"metadata":{"colab":{"collapsed_sections":[],"name":"Homework_4.ipynb","provenance":[],"machine_shape":"hm"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"gpuClass":"standard"},"nbformat":4,"nbformat_minor":0}